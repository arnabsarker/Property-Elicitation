{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_vectors(k, d):\n",
    "    W = np.ones([k, d])\n",
    "    # Generate k vectors for classes\n",
    "    mean = [0] * d\n",
    "    cov = np.eye(d)\n",
    "    for j in range(0, k):\n",
    "        vec = np.random.multivariate_normal(mean, cov)\n",
    "        vec = vec / np.linalg.norm(vec)\n",
    "        W[j, ] = vec\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_argmax_data(k, d, num_samples):\n",
    "    W = generate_class_vectors(k, d)\n",
    "\n",
    "    X = np.zeros([num_samples, d])\n",
    "    Y = np.zeros([num_samples, 1])\n",
    "\n",
    "    mean = [0] * d\n",
    "    cov = np.eye(d)\n",
    "    for i in range(0, num_samples):\n",
    "        vec = np.random.multivariate_normal(mean, cov)\n",
    "        vec = vec / np.linalg.norm(vec)\n",
    "        X[i, ] = vec\n",
    "\n",
    "        true_y = get_closest_vector(W, vec, k)\n",
    "        r = np.random.uniform()\n",
    "        if(r < 0.95):\n",
    "            Y[i, ] = true_y\n",
    "        else:\n",
    "            Y[i, ] = np.random.randint(0, k)\n",
    "\n",
    "    return (W, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "n = 10000\n",
    "d = 3\n",
    "(W, X, Y) = generate_argmax_data(k, d, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('multiclass_train_features_k' + str(k) + 'n' + str(n) + '.csv', X[1:int(n/2), :], delimiter=',')\n",
    "np.savetxt('multiclass_train_labels_k' + str(k) + 'n' + str(n) + '.csv', Y[1:int(n/2), :], delimiter=',')\n",
    "np.savetxt('multiclass_test_features_k' + str(k) + 'n' + str(n) + '.csv', X[int(n/2):, :], delimiter=',')\n",
    "np.savetxt('multiclass_test_labels_k' + str(k) + 'n' + str(n) + '.csv', Y[int(n/2):, :], delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_softmax_data(k, d, num_samples):\n",
    "    W = generate_class_vectors(k, d)\n",
    "\n",
    "    X = np.zeros([num_samples, d])\n",
    "    Y = np.zeros([num_samples, 1])\n",
    "\n",
    "    cov = np.eye(d) * (get_lowest_vector_distance(W) / (1000 * k))\n",
    "    \n",
    "    i = 0\n",
    "    while(i < num_samples):\n",
    "        true_y = np.random.randint(0, k)\n",
    "\n",
    "        vec = np.random.multivariate_normal(W[true_y, ], cov)\n",
    "        vec = vec / np.linalg.norm(vec)\n",
    "        Y_dist = np.exp(np.matmul(W, vec))\n",
    "        Y_dist = Y_dist / np.sum(Y_dist)\n",
    "        if(log_condition(Y_dist, k)) :\n",
    "            label = np.random.choice(np.arange(0, k), p=Y_dist)\n",
    "            Y[i, ] = label\n",
    "            X[i, ] = vec\n",
    "            i = i + 1\n",
    "        \n",
    "\n",
    "    return (W, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_condition(Y_dist, k):\n",
    "    largest = np.amax(Y_dist)\n",
    "    if(Y_dist[~(Y_dist == largest)].size == 0):\n",
    "        return False\n",
    "    second_largest = np.amax(Y_dist[~(Y_dist == largest)])\n",
    "    \n",
    "    # Slight change to condition to allow for more \n",
    "    return largest > ( second_largest + 1 / np.log(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_condition(np.array([0, 1]), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowest_vector_distance(W):\n",
    "    arr = spatial.distance.cdist(W, W)\n",
    "    return np.min(arr[np.nonzero(arr)])\n",
    "\n",
    "\n",
    "def get_closest_vector(W, vec, k):\n",
    "    best_distance = float('inf')\n",
    "    for i in range(0, k):\n",
    "        dist = np.linalg.norm(W[i, ] - vec)\n",
    "        if(dist < best_distance):\n",
    "            best_distance = dist\n",
    "            y = i\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(W, X, Y) = generate_argmax_data(12, 10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns the ith linear classifier using gradient descent\n",
    "def learn_linear_classifier_grad_descent(i, s, X, Y, norm):\n",
    "    d = X.shape[1]\n",
    "    eta = 0.001\n",
    "    precision = 10**(-1)\n",
    "    max_iter = 10**5\n",
    "    \n",
    "    iters = 0\n",
    "    w_hat = np.ones([1, d]) * (i + 1)/d\n",
    "    curr_opt = opt_function(w_hat, args=(i, s, X, Y))\n",
    "    while((iters < max_iter) and (iters == 0 or (curr_opt >= precision))):\n",
    "        prev_opt = curr_opt\n",
    "        prev_w_hat = w_hat\n",
    "        \n",
    "        w_hat = prev_w_hat - eta * obj_gradient(i, s, prev_w_hat, X, Y, norm)\n",
    "        \n",
    "        iters = iters + 1\n",
    "        curr_opt = opt_function(w_hat, args=(i, s, X, Y))\n",
    "        ## eta = (100 + iters)**(-0.75)\n",
    "    \n",
    "    print('Num Iterations: ' + str(iters))\n",
    "    print('Obj Function Val: ' + str(opt_function(w_hat, args=(i, s, X, Y))))\n",
    "    print('Final Vector:' + str(w_hat))\n",
    "    return w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 1\n",
    "w = learn_linear_classifier_grad_descent(1, 2, X_test, Y_test, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.random.rand(500, 2)\n",
    "Y_test = np.zeros([500, 1])\n",
    "for i in range(0, 500):\n",
    "    if(X_test[i, 0] > X_test[i, 1]):\n",
    "        Y_test[i, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = np.matmul(X_test, np.transpose(w))\n",
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(np.round(estimates) - Y_test)) / 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat = learn_all_classifiers(5, X_test, Y_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 14\n",
    "print(X_test[sample, ])\n",
    "print(Y_test[sample, ])\n",
    "print(classify_sample(W_hat, X_test[sample, ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(W_hat, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_gradient(i, s, w, X, Y, norm):\n",
    "    estimates = np.matmul(X, np.transpose(w))\n",
    "    error = estimates - Y\n",
    "    pos_gradient = X * (1 - i / s) * (error > 0)\n",
    "    neg_gradient = X * (i / s) * (error < 0)\n",
    "    pos_gradient = np.mean(pos_gradient, axis=0)\n",
    "    neg_gradient = np.mean(neg_gradient, axis=0)\n",
    "    \n",
    "    lmda = 0.01\n",
    "    reg = w * 0\n",
    "    if(norm == 1):\n",
    "        reg = np.abs(w) / w\n",
    "    elif(norm == 2):\n",
    "        reg = 2 * w \n",
    "    return pos_gradient - neg_gradient + lmda * reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns the ith linear classifier\n",
    "def learn_linear_classifier(i, s, X, Y, norm):\n",
    "    d = X.shape[1]\n",
    "    w_hat = optimize.minimize(opt_function, [1/d]*d, args=(i, s, X, Y, norm), method='Nelder-Mead').x\n",
    "    return w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_function(w_hat, args):\n",
    "    i = args[0]\n",
    "    s = args[1]\n",
    "    X = args[2]\n",
    "    Y = args[3]\n",
    "    estimates = np.matmul(X, np.transpose(w_hat))\n",
    "    error = estimates - Y\n",
    "    pos_error = error * (1 - i / s) * (error > 0)\n",
    "    neg_error = -error * (i / s) * (error < 0)\n",
    "    pos_mean = np.mean(pos_error, axis=0)\n",
    "    neg_mean = np.mean(neg_error, axis=0)\n",
    "    return pos_mean + neg_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_all_classifiers(s, X, Y, norm):\n",
    "    d = X.shape[1]\n",
    "    W_hat = np.zeros([s, d])\n",
    "\n",
    "    for i in range(0, s):\n",
    "        W_hat[i, ] = learn_linear_classifier_grad_descent(i, s, X, Y, norm)\n",
    "\n",
    "    return W_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sample(W_hat, x):\n",
    "    quantiles = np.matmul(W_hat, x)\n",
    "    print(quantiles)\n",
    "    quantiles = np.round(quantiles)\n",
    "    return stats.mode(quantiles)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(W_hat, X, Y):\n",
    "    num_samples = X.shape[0]\n",
    "    total = 0\n",
    "    for i in range(0, num_samples):\n",
    "        if(classify_sample(W_hat, X[i, ]) != Y[i,]):\n",
    "            total += 1\n",
    "    return total / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(W_hat, X, Y):\n",
    "    num_samples = X.shape[0]\n",
    "    total = 0\n",
    "    for i in range(0, num_samples):\n",
    "        print(classify_sample(W_hat, X[i, ]))\n",
    "        print(Y[i, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "k = 32 # For k large enough (> 30 suffices), argmax data satisfies condition\n",
    "d = int(3 * np.log2(k))\n",
    "s = int(3 * np.log2(k))\n",
    "norm = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(W, X, Y) = generate_argmax_data(k, d, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "W_hat = learn_all_classifiers(s, X, Y, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(W_hat, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 50\n",
    "print(X[sample, ])\n",
    "print(Y[sample, ])\n",
    "print(classify_sample(W_hat, X[sample, ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression for comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X, Y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(clf.predict(X) != Y.flatten()) / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OVA SVM\n",
    "from sklearn import svm\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(X, Y.flatten()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lin_clf.predict(X) != Y.flatten()) / num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "k = 100\n",
    "d = int(10 * np.log10(k))\n",
    "s = int(10 * np.log10(k))\n",
    "norm = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
