{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From mord\n",
    "These first few cells are taken directly from the ordinal regression package, with some modifications for the logistic regression with weights for quantile elicitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import scipy\n",
    "from sklearn import base, metrics\n",
    "from sklearn.utils.validation import check_X_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_loss(np.array([0.2, 1, -2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "some ordinal regression algorithms\n",
    "\n",
    "This implements the margin-based ordinal regression methods described\n",
    "in http://arxiv.org/abs/1408.2327\n",
    "\"\"\"\n",
    "\n",
    "def sigmoid(t):\n",
    "    # sigmoid function, 1 / (1 + exp(-t))\n",
    "    # stable computation\n",
    "    idx = t > 0\n",
    "    out = np.zeros_like(t)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "\n",
    "def log_loss(Z):\n",
    "    # stable computation of the logistic loss\n",
    "    idx = Z > 0\n",
    "    out = np.zeros_like(Z)\n",
    "    out[idx] = np.log(1 + np.exp(-Z[idx]))\n",
    "    out[~idx] = (-Z[~idx] + np.log(1 + np.exp(Z[~idx])))\n",
    "    return out\n",
    "\n",
    "def direct_loss(Z):\n",
    "    # Weighted surrogate loss\n",
    "    idx = Z > 0\n",
    "    out = np.zeros_like(Z)\n",
    "    out[idx] = 0\n",
    "    out[~idx] = -Z[~idx]\n",
    "    return out\n",
    "\n",
    "def obj_margin(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Objective function for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "\n",
    "    err = loss_fd.T * log_loss(S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        err *= sample_weight\n",
    "    obj = np.sum(err)\n",
    "    obj += alpha * 0.5 * (np.dot(w, w))\n",
    "    return obj\n",
    "\n",
    "def obj_margin_direct(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Objective function for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "\n",
    "    err = loss_fd.T * direct_loss(S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        err *= sample_weight\n",
    "    obj = np.sum(err)\n",
    "    #obj += alpha * 0.5 * (np.dot(w, w))\n",
    "    return obj\n",
    "\n",
    "def grad_margin(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Gradient for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "    # Alpha[idx] *= -1\n",
    "    # W[idx.T] *= -1\n",
    "\n",
    "    Sigma = S * loss_fd.T * sigmoid(-S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        Sigma *= sample_weight\n",
    "\n",
    "    grad_w = X.T.dot(Sigma.sum(0)) + alpha * w\n",
    "\n",
    "    grad_theta = -Sigma.sum(1)\n",
    "    grad_c = L.T.dot(grad_theta)\n",
    "    return np.concatenate((grad_w, grad_c), axis=0)\n",
    "\n",
    "\n",
    "def threshold_fit(X, y, alpha, n_class, mode='AE',\n",
    "                  max_iter=1000, verbose=False, tol=1e-12,\n",
    "                  sample_weight=None):\n",
    "    \"\"\"\n",
    "    Solve the general threshold-based ordinal regression model\n",
    "    using the logistic loss as surrogate of the 0-1 loss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : string, one of {'AE', '0-1', 'SE'}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = check_X_y(X, y, accept_sparse='csr')\n",
    "    unique_y = np.sort(np.unique(y))\n",
    "    if not np.all(unique_y == np.arange(unique_y.size)):\n",
    "        raise ValueError(\n",
    "            'Values in y must be %s, instead got %s'\n",
    "            % (np.arange(unique_y.size), unique_y))\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # convert from c to theta\n",
    "    L = np.zeros((n_class - 1, n_class - 1))\n",
    "    L[np.tril_indices(n_class-1)] = 1.\n",
    "\n",
    "    if mode == 'AE':\n",
    "        # loss forward difference\n",
    "        loss_fd = np.ones((n_class, n_class - 1))\n",
    "    elif mode == '0-1':\n",
    "        loss_fd = np.diag(np.ones(n_class - 1)) + \\\n",
    "            np.diag(np.ones(n_class - 2), k=-1)\n",
    "        loss_fd = np.vstack((loss_fd, np.zeros(n_class - 1)))\n",
    "        loss_fd[-1, -1] = 1  # border case\n",
    "    elif mode == 'SE':\n",
    "        a = np.arange(n_class-1)\n",
    "        b = np.arange(n_class)\n",
    "        loss_fd = np.abs((a - b[:, None])**2 - (a - b[:, None]+1)**2)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x0 = np.zeros(n_features + n_class - 1)\n",
    "    x0[X.shape[1]:] = np.arange(n_class - 1)\n",
    "    options = {'maxiter' : max_iter, 'disp': verbose}\n",
    "    if n_class > 2:\n",
    "        bounds = [(None, None)] * (n_features + 1) + \\\n",
    "                 [(0, None)] * (n_class - 2)\n",
    "    else:\n",
    "        bounds = None\n",
    "\n",
    "    sol = optimize.minimize(obj_margin, x0, method='L-BFGS-B',\n",
    "        jac=grad_margin, bounds=bounds, options=options,\n",
    "        args=(X, y, alpha, n_class, loss_fd, L, sample_weight),\n",
    "        tol=tol)\n",
    "    if verbose and not sol.success:\n",
    "        print(sol.message)\n",
    "\n",
    "    w, c = sol.x[:X.shape[1]], sol.x[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    return w, theta\n",
    "\n",
    "def threshold_fit_quantile(X, y, alpha, gamma, n_class, mode='QE',\n",
    "                  max_iter=1000, verbose=False, tol=1e-12,\n",
    "                  sample_weight=None):\n",
    "    \"\"\"\n",
    "    Solve the general threshold-based ordinal regression model\n",
    "    using the logistic loss as surrogate of the 0-1 loss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : string, one of {'AE', '0-1', 'Direct'}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = check_X_y(X, y, accept_sparse='csr')\n",
    "    unique_y = np.sort(np.unique(y))\n",
    "    if not np.all(unique_y == np.arange(unique_y.size)):\n",
    "        raise ValueError(\n",
    "            'Values in y must be %s, instead got %s'\n",
    "            % (np.arange(unique_y.size), unique_y))\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # convert from c to theta\n",
    "    L = np.zeros((n_class - 1, n_class - 1))\n",
    "    L[np.tril_indices(n_class-1)] = 1.\n",
    "\n",
    "    if mode == '0-1' or mode == 'Direct':\n",
    "        loss_fd = (1 - gamma) * np.diag(np.ones(n_class - 1)) + \\\n",
    "                  gamma * np.diag(np.ones(n_class - 2), k=-1)\n",
    "        loss_fd = np.vstack((loss_fd, np.zeros(n_class - 1)))\n",
    "        loss_fd[-1, -1] = gamma  # border case\n",
    "    elif mode == 'AE':\n",
    "        # loss forward difference\n",
    "        loss_fd = np.ones((n_class, n_class - 1)) * (1 - gamma)\n",
    "        lower_indices = np.tril_indices(n = n_class, m = (n_class - 1), k = -1)\n",
    "        loss_fd[lower_indices] = gamma\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x0 = np.zeros(n_features + n_class - 1)\n",
    "    x0[X.shape[1]:] = np.arange(n_class - 1)\n",
    "    options = {'maxiter' : max_iter, 'disp': verbose}\n",
    "    if n_class > 2:\n",
    "        bounds = [(None, None)] * (n_features + 1) + \\\n",
    "                 [(0, None)] * (n_class - 2)\n",
    "    else:\n",
    "        bounds = None\n",
    "\n",
    "    if (mode != 'Direct'): \n",
    "        sol = optimize.minimize(obj_margin, x0, method='L-BFGS-B',\n",
    "            jac=grad_margin, bounds=bounds, options=options,\n",
    "            args=(X, y, alpha, n_class, loss_fd, L, sample_weight),\n",
    "            tol=tol)\n",
    "    else:\n",
    "        sol = optimize.minimize(obj_margin_direct, x0, method='L-BFGS-B',\n",
    "            bounds=bounds, options=options,\n",
    "            args=(X, y, alpha, n_class, loss_fd, L, sample_weight),\n",
    "            tol=tol)\n",
    "    \n",
    "    if verbose and not sol.success:\n",
    "        print(sol.message)\n",
    "\n",
    "    w, c = sol.x[:X.shape[1]], sol.x[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    return w, theta\n",
    "\n",
    "\n",
    "def threshold_predict(X, w, theta):\n",
    "    \"\"\"\n",
    "    Class numbers are assumed to be between 0 and k-1\n",
    "    \"\"\"\n",
    "    tmp = theta[:, None] - np.asarray(X.dot(w))\n",
    "    pred = np.sum(tmp < 0, axis=0).astype(np.int)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def threshold_proba(X, w, theta):\n",
    "    \"\"\"\n",
    "    Class numbers are assumed to be between 0 and k-1. Assumes\n",
    "    the `sigmoid` link function is used.\n",
    "    \"\"\"\n",
    "    eta = theta[:, None] - np.asarray(X.dot(w), dtype=np.float64)\n",
    "    prob = np.pad(\n",
    "        sigmoid(eta).T,\n",
    "        pad_width=((0, 0), (1, 1)),\n",
    "        mode='constant',\n",
    "        constant_values=(0, 1))\n",
    "    return np.diff(prob)\n",
    "\n",
    "\n",
    "class LogisticAT(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model (All-Threshold variant)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha: float\n",
    "        Regularization parameter. Zero is no regularization, higher values\n",
    "        increate the squared l2 regularization.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    J. D. M. Rennie and N. Srebro, \"Loss Functions for Preference Levels :\n",
    "    Regression with Discrete Ordered Labels,\" in Proceedings of the IJCAI\n",
    "    Multidisciplinary Workshop on Advances in Preference Handling, 2005.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1., verbose=0, max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(np.int)\n",
    "        if np.abs(_y - y).sum() > 0.1:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit(\n",
    "            X, y_tmp, self.alpha, self.n_class_, mode='AE',\n",
    "            verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return -metrics.mean_absolute_error(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "class LogisticIT(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model\n",
    "    (Immediate-Threshold variant).\n",
    "\n",
    "    Contrary to the OrdinalLogistic model, this variant\n",
    "    minimizes a convex surrogate of the 0-1 loss, hence\n",
    "    the score associated with this object is the accuracy\n",
    "    score, i.e. the same score used in multiclass\n",
    "    classification methods (sklearn.metrics.accuracy_score).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha: float\n",
    "        Regularization parameter. Zero is no regularization, higher values\n",
    "        increate the squared l2 regularization.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    J. D. M. Rennie and N. Srebro, \"Loss Functions for Preference Levels :\n",
    "    Regression with Discrete Ordered Labels,\" in Proceedings of the IJCAI\n",
    "    Multidisciplinary Workshop on Advances in Preference Handling, 2005.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1., verbose=0, max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(np.int)\n",
    "        if np.abs(_y - y).sum() > 0.1:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit(\n",
    "            X, y_tmp, self.alpha, self.n_class_,\n",
    "            mode='0-1', verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return metrics.accuracy_score(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "class LogisticSE(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model\n",
    "    (Squared Error variant).\n",
    "\n",
    "    Contrary to the OrdinalLogistic model, this variant\n",
    "    minimizes a convex surrogate of the 0-1 (?) loss ...\n",
    "\n",
    "    TODO: double check this description (XXX)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha: float\n",
    "        Regularization parameter. Zero is no regularization, higher values\n",
    "        increase the squared l2 regularization.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    J. D. M. Rennie and N. Srebro, \"Loss Functions for Preference Levels :\n",
    "    Regression with Discrete Ordered Labels,\" in Proceedings of the IJCAI\n",
    "    Multidisciplinary Workshop on Advances in Preference Handling, 2005.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1., verbose=0, max_iter=100000):\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(np.int)\n",
    "        if np.abs(_y - y).sum() > 1e-3:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit(\n",
    "            X, y_tmp, self.alpha, self.n_class_,\n",
    "            mode='SE', verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return -metrics.mean_squared_error(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)\n",
    "\n",
    "class LogisticQuantileIT(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model for quantile estimation\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=0.5, alpha=1., verbose=0, max_iter=1000):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(np.int)\n",
    "        if np.abs(_y - y).sum() > 0.1:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit_quantile(\n",
    "            X, y_tmp, self.alpha, self.gamma, self.n_class_,\n",
    "            mode='0-1', verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return metrics.accuracy_score(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)\n",
    "    \n",
    "class LogisticQuantileAT(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model for quantile estimation\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=0.5, alpha=1., verbose=0, max_iter=1000):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(np.int)\n",
    "        if np.abs(_y - y).sum() > 0.1:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit_quantile(\n",
    "            X, y_tmp, self.alpha, self.gamma, self.n_class_,\n",
    "            mode='AE', verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return metrics.accuracy_score(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)\n",
    "    \n",
    "class LogisticQuantileDirect(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model for quantile estimation\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=0.5, alpha=1., verbose=0, max_iter=1000):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(np.int)\n",
    "        if np.abs(_y - y).sum() > 0.1:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit_quantile(\n",
    "            X, y_tmp, self.alpha, self.gamma, self.n_class_,\n",
    "            mode='Direct', verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return metrics.accuracy_score(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf6 = LogisticQuantileAT(gamma=a, alpha=1.)\n",
    "clf6.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticQuantileAT, gamma=' + str(a) + ' %s' %\n",
    "      metrics.mean_absolute_error(clf6.predict(X_test), y_test))\n",
    "\n",
    "clf7 = LogisticQuantileDirect(gamma=a, alpha=1.)\n",
    "clf7.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticQuantileDirect, gamma=' + str(a) + ' %s' %\n",
    "      metrics.mean_absolute_error(clf7.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Elicitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Quantile to be elicited\n",
    "a = 0.5\n",
    "\n",
    "X_train = pd.read_csv('quantile_synthetic_features.csv')\n",
    "y = pd.read_csv('quantile_synthetic_labels.csv')\n",
    "y_train = np.array(y.iloc[:, 0]).astype(int)\n",
    "\n",
    "X_test = pd.read_csv('quantile_synthetic_testfeatures' + str(a*100) + '.csv')\n",
    "y = pd.read_csv('quantile_synthetic_testlabels' + str(a*100) + '.csv')\n",
    "y_test = np.array(y.iloc[:, 0]).astype(int)\n",
    "\n",
    "clf1 = linear_model.LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    multi_class='multinomial')\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "print('Mean Absolute Error of LogisticRegression: %s' %\n",
    "      metrics.mean_absolute_error(clf1.predict(X_test), y_test))\n",
    "\n",
    "clf2 = LogisticAT(alpha=1.)\n",
    "clf2.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticAT %s' %\n",
    "      metrics.mean_absolute_error(clf2.predict(X_test), y_test))\n",
    "\n",
    "clf3 = LogisticIT(alpha=1.)\n",
    "clf3.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticIT %s' %\n",
    "      metrics.mean_absolute_error(clf3.predict(X_test), y_test))\n",
    "\n",
    "clf4 = LogisticSE(alpha=1.)\n",
    "clf4.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticSE %s' %\n",
    "      metrics.mean_absolute_error(clf4.predict(X_test), y_test))\n",
    "\n",
    "clf5 = LogisticQuantileIT(gamma=a, alpha=1.)\n",
    "clf5.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticQuantileIT, gamma=' + str(a) + ' %s' %\n",
    "      metrics.mean_absolute_error(clf5.predict(X_test), y_test))\n",
    "\n",
    "clf6 = LogisticQuantileAT(gamma=a, alpha=1.)\n",
    "clf6.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticQuantileAT, gamma=' + str(a) + ' %s' %\n",
    "      metrics.mean_absolute_error(clf6.predict(X_test), y_test))\n",
    "\n",
    "clf7 = LogisticQuantileDirect(gamma=a, alpha=1.)\n",
    "clf7.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticQuantileDirect, gamma=' + str(a) + ' %s' %\n",
    "      metrics.mean_absolute_error(clf7.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "full = np.zeros((9999, 2))\n",
    "full[:, 0] = clf7.predict(X_test)\n",
    "full[:,1] = y_test\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "n = 10000\n",
    "X_train = pd.read_csv('multiclass_train_features_k' + str(k) + 'n' + str(n) + '.csv')\n",
    "y = pd.read_csv('multiclass_train_labels_k' + str(k) + 'n' + str(n) + '.csv')\n",
    "y_train = np.array(y.iloc[:, 0]).astype(int)\n",
    "\n",
    "X_test = pd.read_csv('multiclass_test_features_k' + str(k) + 'n' + str(n) + '.csv')\n",
    "y = pd.read_csv('multiclass_test_labels_k' + str(k) + 'n' + str(n) + '.csv')\n",
    "y_test = np.array(y.iloc[:, 0]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITs = []\n",
    "IT_predictions = []\n",
    "ATs = []\n",
    "AT_predictions = []\n",
    "\n",
    "s = 10\n",
    "for i in range(1, s):\n",
    "    a = i / s\n",
    "    clf5 = LogisticQuantileIT(gamma=a, alpha=1.)\n",
    "    clf5.fit(X_train, y_train)\n",
    "    ITpredictions = clf5.predict(X_test)\n",
    "    print('Mean Absolute Error of LogisticQuantileIT, gamma=' + str(a) + ' %s' %\n",
    "          metrics.mean_absolute_error(ITpredictions, y_test))\n",
    "    ITs.append(clf5)\n",
    "    IT_predictions.append(ITpredictions)\n",
    "\n",
    "    clf6 = LogisticQuantileAT(gamma=a, alpha=1.)\n",
    "    clf6.fit(X_train, y_train)\n",
    "    ATpredictions = clf6.predict(X_test)\n",
    "    print('Mean Absolute Error of LogisticQuantileAT, gamma=' + str(a) + ' %s' %\n",
    "          metrics.mean_absolute_error(ATpredictions, y_test))\n",
    "    ATs.append(clf6)\n",
    "    AT_predictions.append(ATpredictions)\n",
    "\n",
    "#     clf7 = LogisticQuantileDirect(gamma=a, alpha=1.)\n",
    "#     clf7.fit(X_train, y_train)\n",
    "#     print('Mean Absolute Error of LogisticQuantileDirect, gamma=' + str(a) + ' %s' %\n",
    "#           metrics.mean_absolute_error(clf7.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression for comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(clf.predict(X_test) != y_test.flatten()) / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(clf.predict(X_test), y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "full = np.zeros((4999, 2))\n",
    "full[:, 0] = clf.predict(X_test)\n",
    "full[:,1] = y_test\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OVA SVM\n",
    "from sklearn import svm\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(X_train, y_train.flatten()) \n",
    "\n",
    "print(sum(lin_clf.predict(X_test) != y_test.flatten()) / len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "full = np.zeros((4999, 2))\n",
    "full[:, 0] = AT_predictions[1]\n",
    "full[:,1] = y_test\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(AT_predictions[4] != y_test.flatten()) / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT_preds = np.zeros((4999, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    AT_preds[:, i] = AT_predictions[i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(scipy.stats.mode(AT_preds.T).mode!= y_test.flatten()) / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(scipy.stats.mode(AT_preds.T).mode, y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_preds = np.zeros((4999, 10))\n",
    "for i in range(1, 10):\n",
    "    IT_preds[:, i-1] = IT_predictions[i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(scipy.stats.mode(IT_preds.T).mode!= y_test.flatten()) / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
