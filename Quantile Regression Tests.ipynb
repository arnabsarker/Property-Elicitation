{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From mord\n",
    "These first few cells are taken directly from the ordinal regression package, with some modifications for the logistic regression with weights for quantile elicitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import scipy\n",
    "from sklearn import base, metrics\n",
    "from sklearn.utils.validation import check_X_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticAT(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model (All-Threshold variant)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha: float\n",
    "        Regularization parameter. Zero is no regularization, higher values\n",
    "        increate the squared l2 regularization.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    J. D. M. Rennie and N. Srebro, \"Loss Functions for Preference Levels :\n",
    "    Regression with Discrete Ordered Labels,\" in Proceedings of the IJCAI\n",
    "    Multidisciplinary Workshop on Advances in Preference Handling, 2005.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1., verbose=0, max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(np.int)\n",
    "        if np.abs(_y - y).sum() > 0.1:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit(\n",
    "            X, y_tmp, self.alpha, self.n_class_, mode='AE',\n",
    "            verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return -metrics.mean_absolute_error(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "class LogisticIT(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model\n",
    "    (Immediate-Threshold variant).\n",
    "\n",
    "    Contrary to the OrdinalLogistic model, this variant\n",
    "    minimizes a convex surrogate of the 0-1 loss, hence\n",
    "    the score associated with this object is the accuracy\n",
    "    score, i.e. the same score used in multiclass\n",
    "    classification methods (sklearn.metrics.accuracy_score).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha: float\n",
    "        Regularization parameter. Zero is no regularization, higher values\n",
    "        increate the squared l2 regularization.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    J. D. M. Rennie and N. Srebro, \"Loss Functions for Preference Levels :\n",
    "    Regression with Discrete Ordered Labels,\" in Proceedings of the IJCAI\n",
    "    Multidisciplinary Workshop on Advances in Preference Handling, 2005.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1., verbose=0, max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(np.int)\n",
    "        if np.abs(_y - y).sum() > 0.1:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit(\n",
    "            X, y_tmp, self.alpha, self.n_class_,\n",
    "            mode='0-1', verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return metrics.accuracy_score(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "class LogisticSE(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model\n",
    "    (Squared Error variant).\n",
    "\n",
    "    Contrary to the OrdinalLogistic model, this variant\n",
    "    minimizes a convex surrogate of the 0-1 (?) loss ...\n",
    "\n",
    "    TODO: double check this description (XXX)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha: float\n",
    "        Regularization parameter. Zero is no regularization, higher values\n",
    "        increase the squared l2 regularization.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    J. D. M. Rennie and N. Srebro, \"Loss Functions for Preference Levels :\n",
    "    Regression with Discrete Ordered Labels,\" in Proceedings of the IJCAI\n",
    "    Multidisciplinary Workshop on Advances in Preference Handling, 2005.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1., verbose=0, max_iter=100000):\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(np.int)\n",
    "        if np.abs(_y - y).sum() > 1e-3:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit(\n",
    "            X, y_tmp, self.alpha, self.n_class_,\n",
    "            mode='SE', verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return -metrics.mean_squared_error(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This implements the margin-based ordinal regression methods described\n",
    "in http://arxiv.org/abs/1408.2327\n",
    "\"\"\"\n",
    "\n",
    "def sigmoid(t):\n",
    "    # sigmoid function, 1 / (1 + exp(-t))\n",
    "    # stable computation\n",
    "    idx = t > 0\n",
    "    out = np.zeros_like(t)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "\n",
    "def log_loss(Z):\n",
    "    # stable computation of the logistic loss\n",
    "    idx = Z > 0\n",
    "    out = np.zeros_like(Z)\n",
    "    out[idx] = np.log(1 + np.exp(-Z[idx]))\n",
    "    out[~idx] = (-Z[~idx] + np.log(1 + np.exp(Z[~idx])))\n",
    "    return out\n",
    "\n",
    "def direct_loss(Z):\n",
    "    # Weighted surrogate loss\n",
    "    idx = Z > 0\n",
    "    out = np.zeros_like(Z)\n",
    "    out[idx] = 0\n",
    "    out[~idx] = -Z[~idx]\n",
    "    return out\n",
    "\n",
    "def obj_margin(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Objective function for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "\n",
    "    err = loss_fd.T * log_loss(S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        err *= sample_weight\n",
    "    obj = np.sum(err)\n",
    "    obj += alpha * 0.5 * (np.dot(w, w))\n",
    "    return obj\n",
    "\n",
    "def obj_margin_direct(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Objective function for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "\n",
    "    err = loss_fd.T * direct_loss(S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        err *= sample_weight\n",
    "    obj = np.sum(err)\n",
    "    #obj += alpha * 0.5 * (np.dot(w, w))\n",
    "    return obj\n",
    "\n",
    "def grad_margin(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Gradient for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "    # Alpha[idx] *= -1\n",
    "    # W[idx.T] *= -1\n",
    "\n",
    "    Sigma = S * loss_fd.T * sigmoid(-S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        Sigma *= sample_weight\n",
    "\n",
    "    grad_w = X.T.dot(Sigma.sum(0)) + alpha * w\n",
    "\n",
    "    grad_theta = -Sigma.sum(1)\n",
    "    grad_c = L.T.dot(grad_theta)\n",
    "    return np.concatenate((grad_w, grad_c), axis=0)\n",
    "\n",
    "\n",
    "def threshold_fit(X, y, alpha, n_class, mode='AE',\n",
    "                  max_iter=1000, verbose=False, tol=1e-12,\n",
    "                  sample_weight=None):\n",
    "    \"\"\"\n",
    "    Solve the general threshold-based ordinal regression model\n",
    "    using the logistic loss as surrogate of the 0-1 loss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : string, one of {'AE', '0-1', 'SE'}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = check_X_y(X, y, accept_sparse='csr')\n",
    "    unique_y = np.sort(np.unique(y))\n",
    "    if not np.all(unique_y == np.arange(unique_y.size)):\n",
    "        raise ValueError(\n",
    "            'Values in y must be %s, instead got %s'\n",
    "            % (np.arange(unique_y.size), unique_y))\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # convert from c to theta\n",
    "    L = np.zeros((n_class - 1, n_class - 1))\n",
    "    L[np.tril_indices(n_class-1)] = 1.\n",
    "\n",
    "    if mode == 'AE':\n",
    "        # loss forward difference\n",
    "        loss_fd = np.ones((n_class, n_class - 1))\n",
    "    elif mode == '0-1':\n",
    "        loss_fd = np.diag(np.ones(n_class - 1)) + \\\n",
    "            np.diag(np.ones(n_class - 2), k=-1)\n",
    "        loss_fd = np.vstack((loss_fd, np.zeros(n_class - 1)))\n",
    "        loss_fd[-1, -1] = 1  # border case\n",
    "    elif mode == 'SE':\n",
    "        a = np.arange(n_class-1)\n",
    "        b = np.arange(n_class)\n",
    "        loss_fd = np.abs((a - b[:, None])**2 - (a - b[:, None]+1)**2)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x0 = np.zeros(n_features + n_class - 1)\n",
    "    x0[X.shape[1]:] = np.arange(n_class - 1)\n",
    "    options = {'maxiter' : max_iter, 'disp': verbose}\n",
    "    if n_class > 2:\n",
    "        bounds = [(None, None)] * (n_features + 1) + \\\n",
    "                 [(0, None)] * (n_class - 2)\n",
    "    else:\n",
    "        bounds = None\n",
    "\n",
    "    sol = optimize.minimize(obj_margin, x0, method='L-BFGS-B',\n",
    "        jac=grad_margin, bounds=bounds, options=options,\n",
    "        args=(X, y, alpha, n_class, loss_fd, L, sample_weight),\n",
    "        tol=tol)\n",
    "    if verbose and not sol.success:\n",
    "        print(sol.message)\n",
    "\n",
    "    w, c = sol.x[:X.shape[1]], sol.x[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    return w, theta\n",
    "\n",
    "def threshold_fit_quantile(X, y, alpha, gamma, n_class, mode='QE',\n",
    "                  max_iter=1000, verbose=False, tol=1e-12,\n",
    "                  sample_weight=None):\n",
    "    \"\"\"\n",
    "    Solve the general threshold-based ordinal regression model\n",
    "    using the logistic loss as surrogate of the 0-1 loss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : string, one of {'AE', '0-1', 'Direct'}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = check_X_y(X, y, accept_sparse='csr')\n",
    "    unique_y = np.sort(np.unique(y))\n",
    "    if not np.all(unique_y == np.arange(unique_y.size)):\n",
    "        raise ValueError(\n",
    "            'Values in y must be %s, instead got %s'\n",
    "            % (np.arange(unique_y.size), unique_y))\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # convert from c to theta\n",
    "    L = np.zeros((n_class - 1, n_class - 1))\n",
    "    L[np.tril_indices(n_class-1)] = 1.\n",
    "\n",
    "    if mode == '0-1' or mode == 'Direct':\n",
    "        loss_fd = (1 - gamma) * np.diag(np.ones(n_class - 1)) + \\\n",
    "                  gamma * np.diag(np.ones(n_class - 2), k=-1)\n",
    "        loss_fd = np.vstack((loss_fd, np.zeros(n_class - 1)))\n",
    "        loss_fd[-1, -1] = gamma  # border case\n",
    "    elif mode == 'AE':\n",
    "        # loss forward difference\n",
    "        loss_fd = np.ones((n_class, n_class - 1)) * (1 - gamma)\n",
    "        lower_indices = np.tril_indices(n = n_class, m = (n_class - 1), k = -1)\n",
    "        loss_fd[lower_indices] = gamma\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x0 = np.zeros(n_features + n_class - 1)\n",
    "    x0[X.shape[1]:] = np.arange(n_class - 1)\n",
    "    options = {'maxiter' : max_iter, 'disp': verbose}\n",
    "    if n_class > 2:\n",
    "        bounds = [(None, None)] * (n_features + 1) + \\\n",
    "                 [(0, None)] * (n_class - 2)\n",
    "    else:\n",
    "        bounds = None\n",
    "\n",
    "    if (mode != 'Direct'): \n",
    "        sol = optimize.minimize(obj_margin, x0, method='L-BFGS-B',\n",
    "            jac=grad_margin, bounds=bounds, options=options,\n",
    "            args=(X, y, alpha, n_class, loss_fd, L, sample_weight),\n",
    "            tol=tol)\n",
    "    else:\n",
    "        sol = optimize.minimize(obj_margin_direct, x0, method='L-BFGS-B',\n",
    "            bounds=bounds, options=options,\n",
    "            args=(X, y, alpha, n_class, loss_fd, L, sample_weight),\n",
    "            tol=tol)\n",
    "    \n",
    "    if verbose and not sol.success:\n",
    "        print(sol.message)\n",
    "\n",
    "    w, c = sol.x[:X.shape[1]], sol.x[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    return w, theta\n",
    "\n",
    "\n",
    "def threshold_predict(X, w, theta):\n",
    "    \"\"\"\n",
    "    Class numbers are assumed to be between 0 and k-1\n",
    "    \"\"\"\n",
    "    tmp = theta[:, None] - np.asarray(X.dot(w))\n",
    "    pred = np.sum(tmp < 0, axis=0).astype(np.int)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def threshold_proba(X, w, theta):\n",
    "    \"\"\"\n",
    "    Class numbers are assumed to be between 0 and k-1. Assumes\n",
    "    the `sigmoid` link function is used.\n",
    "    \"\"\"\n",
    "    eta = theta[:, None] - np.asarray(X.dot(w), dtype=np.float64)\n",
    "    prob = np.pad(\n",
    "        sigmoid(eta).T,\n",
    "        pad_width=((0, 0), (1, 1)),\n",
    "        mode='constant',\n",
    "        constant_values=(0, 1))\n",
    "    return np.diff(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticQuantileIT(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model for quantile estimation\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=0.5, alpha=1., verbose=0, max_iter=1000):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(np.int)\n",
    "        if np.abs(_y - y).sum() > 0.1:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit_quantile(\n",
    "            X, y_tmp, self.alpha, self.gamma, self.n_class_,\n",
    "            mode='0-1', verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return metrics.accuracy_score(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)\n",
    "    \n",
    "class LogisticQuantileAT(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model for quantile estimation\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=0.5, alpha=1., verbose=0, max_iter=1000):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(np.int)\n",
    "        if np.abs(_y - y).sum() > 0.1:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit_quantile(\n",
    "            X, y_tmp, self.alpha, self.gamma, self.n_class_,\n",
    "            mode='AE', verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return metrics.accuracy_score(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)\n",
    "    \n",
    "class LogisticQuantileDirect(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model for quantile estimation\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=0.5, alpha=1., verbose=0, max_iter=1000):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(np.int)\n",
    "        if np.abs(_y - y).sum() > 0.1:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit_quantile(\n",
    "            X, y_tmp, self.alpha, self.gamma, self.n_class_,\n",
    "            mode='Direct', verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return metrics.accuracy_score(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Elicitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_quantile(X_test, alpha):\n",
    "    ## Known from data generation\n",
    "    d = 2\n",
    "    w = np.array([1/d]*d) \n",
    "    portions = np.array([3, 7, 10, 15, 19, 21]) * 8\n",
    "    k = 6\n",
    "    \n",
    "    projection_points = X_test.dot(w) * d\n",
    "    return get_class_quantiles(projection_points, portions, k, alpha)\n",
    "    \n",
    "def get_class_quantiles(projection_points, portions, k, alpha):\n",
    "    y = np.matrix(np.zeros(projection_points.size)).T\n",
    "    for i in range(0, projection_points.size):\n",
    "        curr_point = projection_points[i]\n",
    "        likely_class = np.argmax((curr_point - portions) < 0)\n",
    "        if(likely_class == k - 1):\n",
    "            y[i] = k-1\n",
    "        else:\n",
    "            if(likely_class > 0):\n",
    "                prob_up = 1 - (portions[likely_class] - curr_point) / (portions[likely_class] - portions[likely_class - 1])\n",
    "            else:\n",
    "                prob_up = 1 - (portions[likely_class] - curr_point) / (portions[likely_class] - 0)\n",
    "            y[i] = likely_class + 1 * (prob_up > 1 - alpha)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of LogisticRegression: 0.1521\n",
      "Mean Absolute Error of LogisticAT 0.1724\n",
      "Mean Absolute Error of LogisticIT 0.1725\n",
      "Mean Absolute Error of LogisticSE 0.1727\n",
      "Mean Absolute Error of LogisticQuantileIT, gamma=0.7 0.0412\n",
      "Mean Absolute Error of LogisticQuantileAT, gamma=0.7 0.0446\n",
      "Mean Absolute Error of LogisticQuantileDirect, gamma=0.7 0.187\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Quantile to be elicited\n",
    "a = 0.7\n",
    "\n",
    "X_train = pd.read_csv('quantile_synthetic_features.csv', names=['dim1', 'dim2'])\n",
    "y = pd.read_csv('quantile_synthetic_labels.csv', names=['y'])\n",
    "y_train = np.array(y.iloc[:, 0]).astype(int)\n",
    "\n",
    "X_test = pd.read_csv('quantile_synthetic_testfeatures.csv', names=['dim1', 'dim2'])\n",
    "y_test = compute_quantile(X_test, a)\n",
    "\n",
    "clf1 = linear_model.LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    multi_class='multinomial')\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "print('Mean Absolute Error of LogisticRegression: %s' %\n",
    "      metrics.mean_absolute_error(clf1.predict(X_test), y_test))\n",
    "\n",
    "clf2 = LogisticAT(alpha=1.)\n",
    "clf2.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticAT %s' %\n",
    "      metrics.mean_absolute_error(clf2.predict(X_test), y_test))\n",
    "\n",
    "clf3 = LogisticIT(alpha=1.)\n",
    "clf3.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticIT %s' %\n",
    "      metrics.mean_absolute_error(clf3.predict(X_test), y_test))\n",
    "\n",
    "clf4 = LogisticSE(alpha=1.)\n",
    "clf4.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticSE %s' %\n",
    "      metrics.mean_absolute_error(clf4.predict(X_test), y_test))\n",
    "\n",
    "clf5 = LogisticQuantileIT(gamma=a, alpha=1.)\n",
    "clf5.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticQuantileIT, gamma=' + str(a) + ' %s' %\n",
    "      metrics.mean_absolute_error(clf5.predict(X_test), y_test))\n",
    "\n",
    "clf6 = LogisticQuantileAT(gamma=a, alpha=1.)\n",
    "clf6.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticQuantileAT, gamma=' + str(a) + ' %s' %\n",
    "      metrics.mean_absolute_error(clf6.predict(X_test), y_test))\n",
    "\n",
    "clf7 = LogisticQuantileDirect(gamma=a, alpha=1.)\n",
    "clf7.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticQuantileDirect, gamma=' + str(a) + ' %s' %\n",
    "      metrics.mean_absolute_error(clf7.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Threshold Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_clf(coef, theta):\n",
    "    ## From data generation\n",
    "    last_point = 170\n",
    "    \n",
    "    x = np.arange(0, 170, 0.5)\n",
    "    y = x * (coef[0] / coef[1])\n",
    "    plt.plot(x,y)\n",
    "    \n",
    "    for t in theta:\n",
    "        t_y = x * (-coef[1] / coef[0]) + t * (2 / coef[0])\n",
    "        t_y = t_y * (t_y > 0)\n",
    "        plt.plot(x, t_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3XlQ1Hme8Pn3l0NQVA5FOTNRy1sUPIBSy6O0FC2suywRarqf7uma3dieZ/uZ2ehjOmKfjnlinuln5tmN6ImZmJme7d7tbg4t6xar1LrQKhVUFEUEbxO5RS658/juH5kopYCKSZ6fV4Qhpmnmt36Vfkx+xzuV1hohhBC+LcDdCxBCCDH+ZNgLIYQfkGEvhBB+QIa9EEL4ARn2QgjhB2TYCyGEH5BhL4QQfkCGvRBC+AEZ9kII4QeC3L0AgOnTp+ukpCR3L0MIIbxKWVlZi9Y6+nHu6xHDPikpidOnT7t7GUII4VWUUqbHva/sxhFCCD8gw14IIfyADHshhPADMuyFEMIPyLAXQgg/IMNeCCH8gAx7IYTwA1497O+2tnA0//+lo7nJ3UsRQgiP5hEXVY1VbVUlp4s+5NT+D5izIo3UrTswJC9DKeXupQkhhEfx6mG/cM16EhYs5tznn3H+y4NcO11KVFwCKVtfZNG6TYRMmuTuJQohhEdQWmt3r4GVK1fqp80lWMxmLpd8y9mD+2m8epng0IksXr+JlK0vMi0+0UkrFUIIz6GUKtNar3ys+/rKsB+q4eolyg8d4NLxo1gtFgzJKaRuzWL2ilUEBAQ67XmEEMKd/H7YD+rpaKfiq8OUf/4pXXdamBo9g2UvbCf5+S1MnDLV6c8nhBCuJMP+ATarlWunSzl7qIhblecJCp7AgrXrSdmaxcxZc8bteYUQYjzJsB9FS81Nyg8foPLoV1j6+4mbt5DUzCzmpq8mMCjYJWsQQghncOqwV0olAn8EYgAb8Fut9W+UUr8CfgTcdtz1b7TWnzr+zC+AHwJW4D9rrQ+N9hyuHPaD+rq7qCz+kvLDRbQ3NhAWEcnSzZks3ZTJ5KhpLl2LEEKMhbOHfSwQq7U+o5SaApQBrwA7gS6t9f984P6LgEIgDYgDvgDmaa2tIz2HO4b9IG2zcfP8Wc4e3M+N8jICAgKYm76G1K1ZxM1fKOfsCyE81pMM+0eeZ6+1bgAaHF/fVUpVAfGj/JGXgT1a637ghlLqKvbBf+JxFuRqKiCAWSkrmJWygrbGes4d/pQLX3/OpeNHiU6aTWpmFgvWrCd4Qoi7lyqEEGP2RLkEpVQSkAqUOm76sVLqvFLq90qpSMdt8cCtIX+sltH/cfAYkTFxbPizP+cv/vUPvPCjH6OtVg7/2z/x2//1+xzJ+71kGYQQXuuxD9AqpSYDR4C/01p/oJSaCbQAGvhv2Hf1/EAp9S/ACa11nuPP/Q74VGv9/gOP9w7wDoDBYFhhMj32Rym6jNaa2qoLlB8s4sqpE2itmbMijZStWRiTU2QXjxDCrZy6G8fxgMHA+0C+1voDAK1105Df/w+gyPHLWmDoJasJQP2Dj6m1/i3wW7Dvs3+cdbiaUorERckkLkrm7p0Wzn/xGee/PMS106VExiWQKlkGIYSXeJwDtAr4A9Cqtf7JkNtjHfvzUUr9FyBda71LKbUYKOD+AdovgbmeeoD2SQ1mGcoPFtFw9ZJkGYQQbuPss3HWAt8AFdhPvQT4GyAbSMG+G+cm8BdDhv8vgR8AFuAnWuvPRnsObxr2QzVevczZQ0WSZRBCuIVcVOViPZ0dVHx5SLIMQgiXkmHvJsNlGeavWUdq5g7JMgghnE6GvQcYLsuQkpnFPMkyCCGcRIa9B5EsgxBivMiw90CDWYbyQ0VcP3tasgxCiKfm9PPsxdMbNstQPCTLsDWLBWslyyCEGB/yzt6NzH19VH1bzNlDRbTU3CR08hSWbHyBlC3bCZ8R4+7lCSE8nOzG8TJaa+qqKjl7cP+9LMPs5atIzdwhWQYhxIhk2HuxoVmGno52IuMSSNnyIovXS5ZBCPFdMux9wPBZhudJ2ZLFtATJMgghZNj7HMkyCCGGI8PeRw1mGc59/hl379yWLIMQfk6GvY8bMcuwNYuZs59x9/KEEC4iw96PtNwyUX6oSLIMQvghGfZ+qK+7i4tHvuTsIckyCOEvZNj7sWGzDGmrSc3cIVkGIXyM5BL82IhZhhPf3M8yrFlHcEiou5cqhHAheWfvByTLIIRvkt04Ylj3sgyHirhy8vh3swxLlqECAty9RCHEE5BhLx5JsgxCeD8Z9uKxWcxmrpR8y1nJMgjhdWTYizFpvHqZ8sMHqD52xJ5lWLKM1MwdkmUQwkPJsBdPRbIMQngHGfbCKSTLIIRnk2EvnE6yDEJ4Hhn2YtwMZhnKDx+graGeSeERLN28jWWbJcsghKvJsBfjbqQsQ0pmFvHzF0mWQQgXkFyCGHdDswztjQ2UHz4gWQYhPJi8sxdOY+7ro+pYMWcPOrIMYZNZ8vwWyTIIMU5kN45wqxGzDFuzMCanSJZBCCdx6rBXSiUCfwRiABvwW631b5RSUcBeIAm4CezUWrcp+87a3wDbgR7g+1rrM6M9hwx73yVZBiHGj7OHfSwQq7U+o5SaApQBrwDfB1q11r9WSv0ciNRa/0wptR34S+zDPh34jdY6fbTnkGHv+4bLMixa9zypWyXLIMRYjetuHKXUx8A/O35s0Fo3OP5BKNZaz1dK/bvj60LH/S8N3m+kx5Rh71/uZRmOH8VqNmNYsoyUzCzmrEiTLIMQT2Dchr1SKgk4CiwBarTWEUN+r01rHamUKgJ+rbX+1nH7l8DPtNanH3isd4B3AAwGwwqTyfTY6xC+4cEsw5Tp0aRseZElG19g0tRwdy9PCI83LsNeKTUZOAL8ndb6A6VU+wjD/gDw9w8M+59qrctGemx5Z+/fbFYr18pKOXvQnmUIDA5mwZr1kmUQ4hGcfp69UioYeB/I11p/4Li5SSkVO2Q3TrPj9lpg6E7YBKD+8ZYu/FFAYCBz01YzN221I8twgItHv6Ky+Ati5y0gdWsW8zLWSJZBiKfwOAdoFfAH7AdjfzLk9n8E7gw5QBultf6pUupF4MfcP0D7T1rrtNGeQ97ZiwdJlkGIR3P22ThrgW+ACuynXgL8DVAKvAsYgBrgTa11q+Mfh38GMrGfevmfHtxf/yAZ9mIkkmUQYmRyUZXwSUOzDP3d3UQbZ5GyNYuFa9dLlkH4Jb8Z9gN9Fuout2NcMo2AAHmH5y9GyjIse2E7ETMlyyD8h98M+4vH6vn6T9VMmRbKkvXxLFodR+hkOYjnLyTLIPyd3wx7q9XGjfIWKoprqb/STmBwAPPSZpK8IYHoxCnjsFLhqe62tnD+i4Oc/+KgPcsQG0/K1izJMgif5jfDfqiW2i4qjtRyuaQRi9lG7JxwkjcmMDs1msBAeYfnL+5lGQ4V0XBFsgzCt/nlsB/U122m+kQDFcW1dLb0MSl8Aoufi2fxc3GEhYc45TmEd2i8doXyQ0UPZxmWpxEQKFkG4f38etgP0jaNqfIOFcW11FS2EhComLN8BskbEoiZPVVO2fMjw2UZlr2wneTnt0iWQXg1GfYPaG/qoeJILdXHGxjosxJtmELyhnjmrpxJ0AR5h+cvhs0yrF5PaqZkGYR3kmE/goE+C5dPNlFRXEtrfTehYcEsWhvL4nXxTJ02cdyfX3iOoVkGc3+fZBmEV5Jh/whaa+ovt3O+uJYb5bcBSFo6neQNCSQsiJRdPH5EsgzCm8mwfwJ3W/uoPFpH5bf19HWZiYyZRPKGBOZnxDAhVD6P3V9omw3T+bOcHZJleCZtNamSZRAeTIb9GFjMVq6WNVPxdS3NprsEhwayICOW5A3xRMaEuXVtwrXaGxso//xTLnx9WLIMwqPJsH9KjTc6qCiu5erpZmxWTeLCSJI3JGBMni5ZBj9i7u+j6lvJMgjPJcPeSXo6B7j4bR0XjtbT3d4vWQY/pbWmrrqSsweHZBlSV5KauUOyDMKtZNg7mWQZxKDhswwvOrIMsrtPuJYM+3F0L8tQ2ohlQLIM/mrkLMOLTEswuHt5wk/IsHeBe1mGI3V03u6VLIMfezjLsJSUzB2SZRDjToa9C93PMtRRU3mHgADFnOXRJG9MlCyDn+np7KDiq8OcO/ypZBmES8iwd5P2ph4uHKmj6ng9A31WpidOJnlDAvNWSZbBnwxmGcoPFVFzQbIMYnQ2mx7zWX4y7N3swSxDSFgQi9bEsWRdPFOnS5bBnzyUZZg7n9TMHZJl8HN9ZiufVjTwpxITr6bG82fPJo3pcWTYewjJMohB/T3dVBZ/8UCWIZNlm7dJlsGPmO50k19aw77Tt2jrMTNrehg/2TyXl1Pix/R4Muw9kGQZBIySZdj6IvELFssbAB9ksdr4srqZvBIT31xpITBA8cLCmeRmGFk95+k+P1uGvQeTLIMYJFkG39bc2ceeU7coPFlDQ0cfM6eGkJ1mYNcqAzHhzvn/K8PeSzTd6OR88S2uljVjs0iWwV8NZhnKDxZxW7IMXk1rzYlrd8grNXG4sgmLTfPc3OnkpBvZvHAGQU6+FkeGvZexZxnquXC0TrIMfuxeluHQAa6UHrufZdiahXFpqmQZPFhHj5n3ztSSX2ri+u1uwicGs3NlArvTjcyaPn7fscuw91LDZhlWObIMBsky+BPJMniH87Xt5JWY+ORcPX1mGymJEeRmGMlaGkto8Pifbi3D3gcMm2XY4MgyBMk7PH/xUJYhJNSeZcjMkiyDm/QOWNl/rp68UhPnazuYGBzIK6lx5KQbWRLv2ovnZNj7EMkyiEHDZhm2ZjFnRbpkGVzganMX+aUm3i+rpbPPwjMzJvN2hpFXl8czNdQ9u1tl2PsgyTKIQZJlcB2z1cbnF5v40wkTJ67fIThQsXVxDLkZRtJnRbn9751Th71S6vdAFtCstV7iuO1XwI+A2467/Y3W+lPH7/0C+CFgBf6z1vrQoxYhw/7J3MsynGhgoNciWQY/JVmG8VPf3suekzXsOXWL5rv9xEdMZHe6gZ0rE4me4jnfUTt72K8DuoA/PjDsu7TW//OB+y4CCoE0IA74ApintbaO9hwy7MdGsgxi0LBZhq1ZzHt2rWQZHpPNpvnmagt5JSa+rGpCA+vnRfN2hpEN82cQ6IGnQzt9N45SKgkoeoxh/wsArfXfO359CPiV1vrEaI8vw/7pDGYZKopruX6uBbSWLIOf6u/ppvLIl5QfKvpOlmHp5kymRE139/I8Ulv3APvKbpFfWoPpTg9RYRPYuTKRnHQDiVGT3L28Ublq2H8f6AROA3+ttW5TSv0zUKK1znPc73fAZ1rr90Z7fBn2zjNclmHJ+gQWPCtZBn8iWYbRaa05U9NOfomJoooGBiw2ViVFkpthJHNJDCFB3rE71BXDfibQAmjgvwGxWusfKKX+BTjxwLD/VGv9/jCP+Q7wDoDBYFhhMpkeZ73iMUmWQQxqb2qk/PCB+1kGQxIpmVksXLvB77IM3f0WPiqvI6+khqqGTiaHBPFqajw5GQYWxEx19/Ke2LgP+5F+T3bjeCbJMggYzDIcofzgfm7X3CQkLIwlG7eQsuVFn88yXGq8S16JiQ/P1tHVb2FBzBTeftbIyynxTA7x3u94XfHOPlZr3eD4+r8A6VrrXUqpxUAB9w/QfgnMlQO0nkGyDAL8J8vQb7Fy8EIj+SU1nLzZyoTAAF5cGktuhoHlBt84luXss3EKgQ3AdKAJ+K+OX6dg341zE/iLIcP/l8APAAvwE631Z49ahAx715Isgxjki1mGW609FJys4d1Tt7jTPYAhahI56QbeXJlIVNgEdy/PqeSiKvHY7tR1UVFcyyXJMvg1i9nMldJjnD24/ztZhpStLzI90eju5T2S1aYpvmRvxhdfvo0Cnl8wk7efNfLcM767u1KGvXhikmUQg+xZhgNUHz/i8VmGlq5+9p66RUFpDXXtvURPCWHXqkR2pRmIj/D9a01k2IsxGzHLsCGBmDnhPrGfUzweT80yaK05eaOVvNIaDl5owGzVPDt7GrkZRrYsnkmwk5vxnkyGvXAKyTIIcGQZzpyk/OD+IVmGdaRm7nBplqGzz8yHZ+rILzVxuamLKaFBvL48gdwMA8/M8M9jTTLshVNJlkEMulNbw9lDB7h45EuXZRkq6zvIK6nh4/I6egasJMeHk5thYMeyOCZN8N7TJp1Bhr0YFw9mGbTWJCVPZ+lGyTL4m/HOMvSZrRw430BeqYmzNe2EBAXw0rI4cjOMLEuMcMJ/gW+QYS/GnWQZBDycZVBKMTdtNamZWWPKMtxs6Sa/1MS+slrae8zMnh5GToaRN5YnED5JrgV5kAx74TIWs5VrZc2clyyD3xsxy7BmA8GhI2cZLFYbX1bbT5v85koLgQGKLYtm8naGkWfnTJPvGEchw164RdONTiqKa7lS1iRZBj/2uFmGps4+9py8xZ5TNTR09BEzNZTsNAO70hKZOdW/mj1jJcNeuNVDWYYoR5ZhjWQZ/MlwWYZZKSsIWbaeopbJHK66jdWmeW7udHIzjGxaMIMgPzpt0hn8ZthrsxlzQwMTDPLBy57IZrVx41wL57+WLIO/q69r5IPC9+g8e5QQSw+dEyKYuGwdr731KvMSo929PK/lN8O+87PPqPurvyZs3XNE5eQQtnatz0ScfM2DWYaY2eEs3ShZBl937lY7eSUm9p+vp89sY3nCZF4Ov0Ng9TGarnpflsHT+M2wNzc30773Xdre3Yv1dgvBRgNRu3cT/uqrBE71vja1P3goyzB1Aoufi2PxunjJMviI3gErn5yzN+Mr6jqYNCGQl1Piyc0wsDju/pW33pRl8FR+M+wH6YEBOg9/Tlt+Pr1nz6ImTiT8pZeIzNlN6Lx5TlypcBbJMvieq81d5JeaeK+slrt9FubOmExuhpFXl8czNXTkYzUPZRmmRbPshW0kb9rq1iyDN/C7YT9Ub2UlbfkFdBYVoQcGmJSWRmRODlM2PY8KkvO/PVF7Uw8XjtZRdVyyDN7GbLVxuLKJvBITJ67fIThQkbkkltx0A2mzop7oH+2RsgwpW7OImTN3HP8rvJdfD/tBlrY22t97j7bCQiz1DQTFxBC5axcRb75B0LRpTn0u4RySZfAe9e29FJ6sYc+pW9y+2098xER2pxvYuTKR6ClPvzvuoSzDM/NJzcxibsZagoLljK5BMuyH0FYrXcXFtObl0XOiBBUczNTt24nMzWFicvK4PKd4OiNmGTYkkLBQsgzuYrNpjl65TV5JDV9VN6GBDfOieftZI+vnzSBwHK6lGDbLsGkrS1/Y5pQsg7eTYT+C/mvXaMsvoOOjj7D19BC6dClRuTlMycwkYIJvfYKNr3gwyxAxcxLJGxJYkBHDhImyW84VWrsH2Hf6FgUnazDd6WFa2AR2rkpkd5qBxKhJLlmDttkwVZRz9uD+72YZtmYRv/DJswy+Qob9I1i7uuj46GPa8vMZuHGDwKgoIna+SeSuXQTH+PYHL3urh7IMIYEseFayDONFa82ZmjbySmo4UNHAgMVGWlIUORkGMpfEEBLkvmMp7U2NnPv8Uy58dZi+7q7HzjL4Ihn2j0nbbHSfOEFbfgFdX38NAQFM2byZyJzdTFq1ym/fLXi6B7MMCQvsWYakpZJleFpd/RY+OltHfmkNVQ2dTA4J4tXUeHIzjMyP8awL4UbMMrywnYiYWHcvzyVk2I/BQG0tbYWFtL/3PraODkLmziUyJ4fwl3YQMMk136qKJyNZBue51HiXvBITH56to6vfwsLYqeRmGHglJZ6wEM/eXTZclmF26kpStmaRtDTVpy+0lGH/FGy9vXQeOEBrXj791dUETJlCxGuvEbk7mwlGucLPEw1mGSqKa6m7LFmGx9VvsXLwQiN5JSZO3WxjQlAAWcmx5GQYWW6I8MrvbO+2tnD+i4Oc/+IgPR3tRMbGkbLlRRZv2EzIJN/b3SfD3gm01vSePUtbXh6dhz8Hq1WyDF5AsgyPdqu1h/zSGvadvsWd7gGM0yaRk27gjRWJRIX5xokKFrOZK6XHOHuoiIbL1T6bZZBh72Tmpmba35Usgzfp7zFTdXyYLMNz8YRF+F+WwWrTFF+yN+OLL99GAZsWziQ3w8hzz/j2sY6m61c5e7DoXpYhcfFSUjN9I8sgw36cSJbB+2ibpuZiK+e/rvXLLMPtu/28e/oWBaU11LX3Ej0lhOxViexKMxAX4V8XqvV0dnDh688pP3yAuy2+kWWQYe8CvZWVtBUU0Fl0AN3fL1kGL9De3MOFI76fZdBaU3qjlbwSE4cqGzFbNavnTCM3w8gLi2YS7OfN+PtZhiJqLpzz6iyDDHsXsrS10fH++7QVFGKur5csgxcYNsuwOo4l6707y9DZZ+bDM3XklZi40tzF1NAgXl+RQE66kWdmTHb38jzScFmGlMws5nlJlkGGvRsMZhna8vPpPn7CkWXYRmRODhOXLnX38sQwfCXLcKGug/xSEx+X19MzYGVpQji56UZ2LItjog99xzKeRswybN7GlGmem2WQYe9mw2YZcnYzZds2yTJ4qLutfVR+U8fFb+vpvev5WYY+s5Wi8w3klZgov9VOaHAALy2LIzfDyNKECHcvz2t5W5bBqcNeKfV7IAto1lovcdwWBewFkoCbwE6tdZuyb4nfANuBHuD7Wuszj1qErw37QZJl8D5Ws42rZU2cL66j+Wanx2UZbrZ0k19qYl9ZLe09ZmZHh5GTbuSN5QmET/L83Q7exBuyDM4e9uuALuCPQ4b9PwCtWutfK6V+DkRqrX+mlNoO/CX2YZ8O/EZrnf6oRfjqsB80bJZh0yYic3Mky+DBPCXLYLHa+KKqmfxSE99caSEoQLFl8Uxy0408O2eavH7G2b0sw6Eibptu2LMMG14gZcuLbs8yOH03jlIqCSgaMuwvARu01g1KqVigWGs9Xyn1746vCx+832iP7+vDfijJMnifns4BLh6rp/JoHV1trssyNHX22ZvxJ2/R2NlHbHgo2WkGdq1KZMZUz3hn6U+01tRdukj5wSKunDyOzWZze5bBFcO+XWsdMeT327TWkUqpIuDXWutvHbd/CfxMaz3qJPenYT/I1ttL56ef2rMMVVWSZfACw2UZ5q6ayVInZhm01hy/doe8EhOHLzZhtWnWzYsmN93A8wtmEOTnp016CnuW4RDnv/iMno52ImJiSdmSxZKNrs0yuHPYHwD+/oFh/1Otddkwj/kO8A6AwWBYYTKZHme9Pud+liGfzsOHJcvgJYbLMiRvjGdO6owxZRk6eszsK7Nf/HS9pZuIScHsXGlvxidNd/+xAjE8q8XM5ZIHswwbSdnyItMNSeP+/LIbx0s9lGUwGIjcnU3Ea69JlsFDPW2W4dytdv5UYmL/uXr6LTaWGyLIzTCyPTmW0GA5bdKbDJtl2JrFnJXjl2VwxbD/R+DOkAO0UVrrnyqlXgR+zP0DtP+ktU571OPLsP+uEbMMu3cTOl+yDJ5oMMtQUVyL6cLoWYaeAQv7z9WTV1JDRV0HkyYE8kpqPLnpRhbFyT/q3s6VWQZnn41TCGwApgNNwH8FPgLeBQxADfCm1rrVcerlPwOZ2E+9/E+P2l8PMuxHI1kG7zNSliEwKYzCslreP1PL3T4L82ZOJjfDyKup8UwJldMmfc1DWYagIOavXkdq5g6nZRnkoiofNHyW4S0i3nxTsgweytxv5eKJBkoPmzC39tOrNJUhVqKWRvHWxtmsSvKeq3TF0xmvLIMMex+mrVa6jhyhLS9PsgwerL69137a5Klb3O7sJ3XiRDYHT0TV9aI1XpllEE/PnmX4ypFlqGNSeARrduaydHPmmB5Phr2f6L92jbaCQjo+/FCyDB7AZtMcvXKbvJIavqpuQgMb588gN8PA+nkzCAxQI2QZ4lmQEeuRWQYxPu5lGQ4VMTdtNUs2bB7T48iw9zOSZXCv1u6Be834mtYepk+ewM6ViWSnGUiMGv5CuWGzDBkxLNmQQFSsnGopHo8Mez+ltab7+PGHsww5OUxKkyyDM2mtOVPTRl5JDQcqGhiw2EibFUVuhpHMxTFMeIJz7T0lyyC8jwx7IVmGcdLVb+Gjs/ZmfHXjXSaHBPH68nhyMozMm/l0V9G6K8sgvJcMe3GPZBmco7qxk7wSEx+draer38Ki2KnkZhh5OSWOsBDn7mt3RZZB+AYZ9uIhD2UZLBZ7liE3V7IMI+i3WDl4oZE/nTBx2tTGhKAAspbGkpthJDUxwiW7xZydZRC+RYa9GJW5uZn2d/fRtnePZBmGcau1h/zSGvadvsWd7gGM0yaRm27kjRUJRIa55yyn/h4z1ScaqSiupWMMWQbhm2TYi8eiBwbo/Pxz2vIL6D1zxq+zDFab5uvqZvJKTRy5fBsFbF44k9wMI2uf8ZwDpcNlGWYvj2bpMFkG4ftk2Isn5q9Zhtt3+9l7qobCk7eoa+9lxpQQdqUZyE5LJDbcsz98fKQsw9xVMwmWz571CzLsxZj5Q5ZBa03pjVbySkwcqmzEbNWsnjONtzOMbF40k2Ava8ab+61cPtnI+a9raa3vJiQsiEWr41iyPp6p0z37HyzxdGTYi6fmi1mGzj4zH5TVkl9aw5XmLqaGBvHGikRyMgzMiZ7s7uU9Na019VfaqSiu5Xp5C1pryTL4OBn2wqkeyjIkJxOVm+M1WYYLdR3klZj4uLyeXrOVZQnh5GQY2bE0jok+urtDsgz+QYa9GBfDZhnefJPIXW8RHOveD15+UJ/ZStH5BvJKTJTfaic0OICXl8WTm2EkOcG5TXFPZjXbuHqmmfNf10qWwQfJsBfjSmtNz4kTtOble1yW4UZLN/klJvaV1dLRa2ZOdBi5GUZeW55A+ET/vgq16UYnFUdquXJasgy+Qoa9cJmB2jra9xTSvu89rG7KMlisNr6oaiavxMS3V1sIClBsXRxDToaBZ2dPk33VD5Asg+/wm2F/8c5F/vbE35K9IJvMWZmEBMrFJe5i6+uj88ABl2YZGjv62HOqhj0nb9HY2UdceCjZaQbeWpXIjKmh4/KcvkSyDN7Pb4Z9aUMp/730v3O94zqRIZG8Pu91ds7bSexkz9p/7E/sWYYcDkDyAAAV90lEQVRy2vLyxiXLYLNpjl+7Q16Jic+rmrDaNOvmRfN2hpGN86MJ8rLTJj2FZBm8k98Me3CcM91YSmFVIcW1xQBsTNxI9oJs0mLS5Ft4N3JmlqG9Z4D3HKdN3mjpJnJSMDtXJrI73YBxmhxodBbJMngXvxr2Q9V31fPupXd5/8r7tPe3Myd8DtkLstkxZweTgiXr6y7DZhl27CAyJ2fULIPWmnO19tMm95+rp99iY4UxktwMA9uWxBIa7JunTXqC72QZKu8QoCTL4In8dtgP6rP0cfDmQQqqCqhqrWJy8GRefuZlds3fRVJ4ktOeRzy5h7IMq1bZswybN93LMvQMWPikvJ68UhMX6jqZNCGQV1LjyU03sihOQm2u1t7cw4WjdVQdkyyDp/H7YT9Ia8252+corC7ksOkwFpuFNXFryF6Qzdr4tQQGyAvVXSxtbXR88IE9y1BXR1BMDLasV/ggbhUFl+9yt8/C/JlTyM0w8EpqPFNC5SwRd3soyzApiIVr4kiWLIPbyLAfRktvC/su72PfpX3c7r1NwuQEdi3YxSvPvEJ4iP9cZONp+vvNHMv/mJ49e5hTU4k5IJDrS54l8YffY/mWNbK7wAONlGVI3hBP4oIolJyz7zIy7Edhtpn5suZLCqsKOdN8htDAUF6c/SLZC7KZHzXfJWsQUNfeS2FpDXtO3aKlq5/EqIn8uSGAdRePMFD0iVdmGfxRV1sfF45KlsFdZNg/purWavZU7+HA9QP0WftYPmM5uxfu5nnD8wQHyG4DZ7PZNEeu3Ca/xMRX1c1o4Pn5M8h91sj6udH3ruL0piyDsJMsg3vIsH9CHf0dfHjlQ/Zc2kNdVx0zJs7gzflv8sa8N5g+cbrb1uUr7nT1s6+slvxSE7dae5k+eQJvrUokO81AQuTIZ0l5cpZBjEyyDK4jw36MrDYr39Z9S0F1AcfrjxMUEMTWpK1kL8hm6fSlMlyegNaaMlMbeSUmPq1oZMBqI31WFLkZRrYujmHCE16oM2KWYUcWAWHyztETSZZh/Mmwd4IbHTfYe2kvH139iG5zN4umLWL3gt2SZXiErn4LH56tI7/ERHXjXaaEBPHa8nhyMozMm/n0l+APn2V4lcjsbCYkJT39f4BwOpvVxo3zjizDJckyOJMMeyfqNnez/9p+CqsLud5xnYiQCF6f+zpvzX9LsgxDVDd2kldi4sMzdXQPWFkcN5XcDCMvLYsjLMT5B+ruZRny8+k8dOh+liEnh7DnnnvqLIMYH3fquqg4UselkgZHlmEqyRsSmLNcsgxj4bJhr5S6CdwFrIBFa71SKRUF7AWSgJvATq1122iP48nDfpDWmpONJymsLuTrW18DkmXot1j5rKKRvBITp01thAQFkLU0jtwMAymJES7bJs7MMgjXeDDLMNGRZVgiWYYn4uphv1Jr3TLktn8AWrXWv1ZK/RyI1Fr/bLTH8YZhP5S/Zxlq7vSQf9LEvtO1tHYPkDRtEjnpRt5YkUBkmPtOkRxrlkG4j7Zpaqpaqfj6u1mG5A0JxEqW4ZHcPewvARu01g1KqVigWGs96gns3jbsB/Vb+/nsxmd+kWWw2jRfVzfzpxITR6/cJkApNi+cQW6GkTVzPO8si76LF2nNzx81yyA8i2QZnpwrh/0NoA3QwL9rrX+rlGrXWkcMuU+b1jpymD/7DvAOgMFgWGEymca8DnfTWnO+5TwFVQU+l2VovtvHu6duUXjyFnXtvcycGsKuVQay0wzEhHt+M364LEPkrreIePNNgqZNc/fyxDAky/D4XDns47TW9UqpGcDnwF8CnzzOsB/KW9/ZD6elt4X3Lr/Hvkv7aO5t9sosg9aakuut5JWaOHShEYtNs/aZ6eRmGNi0cCbBXtiM11YrXUeO0JaXT/fx46jgYKZu30ZkTg4Tly519/LEMCTL8GhuORtHKfUroAv4EX6yG2c03phl6Owz80FZLXmlNVxt7iJ8YjBvrEggJ93A7OjJ7l6e0/Rfv05bQSEdH36Irbub0ORkInN2M3XbNgJC5OCgJ+pq66Pym3oqv6mTLMMQLhn2SqkwIEBrfdfx9efA3wKbgDtDDtBGaa1/Otpj+eKwH+pS6yUKqwu/k2XIXpjNJsMmj8gyXKizN+M/Lq+n12xlWWIEuekGdiyL8+lmvLWri46PP6Ytv4CB69cly+AFBrMMFcW1NN2QLIOrhv1s4EPHL4OAAq313ymlpgHvAgagBnhTa9062mP5+rAf1NHfwUdXP6KwutDtWYY+s5X95+rJK63h3K12QoMDeCUlnpx0I8kJ3rG7yVnuZRnyC+xZBqUky+AFmm52UlHs31kGuajKww1mGQqrCzlWf8ylWYYbLd3kl5jYV1ZLR6+ZOdFh5GYYeW15AuET3f9dhrtJlsH79N4doPLb+1mGyVEhJK9PYOGaWCZO9u1aqgx7L3Kz4yZ7Lu0Z1yyDxWrji6om8kpq+PZqC0EBiq1LYshNN5IxO0reuQ7DnmX4lNb8PPovSpbBGzyUZQgKYG6ab2cZZNh7oW5zN0XXiiisLuRaxzWnZBkaO/ooPFnDnlM1NHX2Exceyu50AztXJTJjiuefNukJJMvgnfwlyyDD3os9bZbBZtMcu9ZCXomJL6qasWnNurnR5GYYeX7BDAL9ZF/meBgxy/DqqwSG+9dxDm/h61kGGfY+oqGrgb2X9t7LMswOn032gmxemvPSQ1mG9p4B3iurJb+0hhst3USFTeDNlQnkpBkxTPP9hIMr6YEB7n7xBa15+ZJl8BL3sgzFtZgu+E6WQYa9j+m39nPwxkEKqgu4eOfivSzDW/Pfor0jgrySGorO19NvsbHSGEluhpFtyTGEBPnuaZOeou/iRVoLCujcX/TdLMOm51HBcsDbEw1mGaqPN9Df491ZBhn2Pmowy5BXac8y2LBg6ZpHwN01vDxvE28/O4uFsVJ5dIeHsgwzZxKZvUuyDB5sMMtQUVzLnbr7WYYl6+IJj/aOLIMMex91peku+aU1vF9WS5eljdjEc+gpx+mytHpllsEXDZdlmLItk6jcXMkyeCitNQ1X2zn/tfdlGWTY+5ABi41DlfZmfOmNViYEBrA9OYbcDCMrjJFYtMXrsgz+QrIM3sfbsgwy7H1AbVsPhSdr2HuqlpaufhKjJrI7zcjOlQlMmzz8oPD0LIO/kiyD9xkuyzA/I4ZkD8syyLD3Ujab5siV2+SXmPiquhmA5xfMICfDyPq50Y99CbgnZRnEfZJl8E6enGWQYe9l7nT18+7pWgpOmrjV2sv0ySHsWpVIdrqB+IixHyiy2qwcqz9GQVXBvSzDFuMWshdksyx6mQwXNxqoraN97x7a393nyDI848gy7JAsg4fqvTvAxWP1XDjiOVkGGfZeQGvNaVMbeSUmPqtoZMBqI31WFLkZRrYujmGCk6/yu9lxk72X9vLR1Y/oMnexaNoishdks23WNqdlGcSTkyyD9xk2y7BqBks3Jro8yyDD3oN19Vv48Gwd+SUmqhvvMiUkiNcdzfi5M8f/hTJSlmHn/J3ETY4b9+cXwxs2y/Dcc0TlSpbBk92p76Ki2H1ZBhn2HqiqoZO8EhMfna2je8DKkvip5KYbeSkljkkTXH+Uf7gsw4aEDexeuPuxsgxi/AybZcjOJuI1yTJ4KndlGWTYe4g+s5XPLjSQV1JDmamNkKAAdiyLIzfDyLIEz7lEu6GrgXcvv8v7l9+nrb9t1CyDcB3JMnifYbMMqdEkbxyfLIMMezerudND/kkT+07X0to9wKzpYeSkG3hjRQIRkzy3rz1SlmHX/F0khSe5e3l+TbIM3ufBLMO0hMks3ZDA3DTnZRlk2LuB1ab5qrqZvBITR6/cJkApXlg4k9wMI6vnTHP7KVpPYjDLUFhdyKGbh7DYLKyJW0P2gmzWxq8lMMC7+iG+RLIM3mc8swwy7F2o+W4fe0/eovBkDfUdfcycGkJ2moFdqwzEhHt/M76lt4X3L7/Pu5fepbm3WbIMHsKeZThKW34+3ceO3c8y5OQwcdkydy9PDON+lqGO6+W372UZUl9IJG5u5JgeU4b9ONNaU3K9lbxSE4cuNGKxadY+M53cDAObFs4kOND3zpww28x8VfMVhdWFlDWVSZbBg0iWwfsMzTIs3ZjAyu2zxvQ4MuzHSUevmQ/O2JvxV5u7CJ8YzJsrEsjJMDJruv9cCCNZBs9k7eqm4+OP7mcZIiPtWYbsXZJl8FBWsw2r1caE0LGdkSfD3skqajvIKzHxybl6es1WUhIjyM0wkrU0ltBg/91/PZhl2FO9h9quWskyeAitNT0lJbTm5duzDHA/y5Aup9X6Ehn2TtA7YGX/+XryS0ycq+1gYnAgL6fYT5tcEi/7qoe6l2WoLuBYnWQZPIlkGXyb/wz7hvNw4K8g7R1Y9DIEPf3+yeu3u8gvreG9slo6es08M2MyuekGXluRwNRQ2UXxKJJl8EySZfBN/jPsrxfDgb+GO1chLBpWfB9W/gCmPtll/2arjS8uNpFXauLY1TsEByq2LrY349NnRck70zHoMfew/9p+yTJ4GK01veXltOVJlsEX+M+wB7DZ4EYxlP4WLh8EFQALd9jf7RtXwyiDuqGjl8KTt9h7qoamzn7iIyayO93AzpWJRE+Rd6HOoLXmVOMpCqoLJMvgYczNzbTv20f7nr1Ybt+WLIMX8q9hP1TbTTj1OzjzR+hrh5lLIO1HkPwmTLDvn7TZNMeutZBXYuKLqmZsWrN+XjS56UY2LphBoBdd/ORtRsoy7Jizg7Bg2X/sLveyDPkF9JaVDcky7CZ0vpxW68n8d9gPGuiBC+/Z3+03VUBoOH1LdvNR0Db+rcLGzTs9RIVNYOfKRHanGTBMk/6LK42UZXhr/lvMCh/b+cbCOfqqqmjNz7+fZVi5ksjcXMkyeCgZ9g7aZuPy6S/o+fZfWdJxhEBsnA1dxcDyH7L8+dcJkRevW2mtqWipoLC6kIM3D2KxWVgdt5rdC3ZLlsHNrO3ttL//AW0FBfezDLveImLnTskyeBCPGPZKqUzgN0Ag8P9orX890n2dPex7Bix8XF5PXomJyvpOwiYE8mfJIfww9AjTLxVAVxNEzbHv4knZDaGyf9LdHswyxE+OZ9f8Xbw691XJMriRZBk8m9uHvVIqELgMvADUAqeAbK31xeHu76xhf6XpLnklJj44U8fdfgsLYqaQm2HkldR4Joc4rlCzDEDVJ3Dyt3CrFILDYNlbsOpHMHPRU69BPB3JMnguyTJ4Hk8Y9s8Cv9Jab3X8+hcAWuu/H+7+TzPsByw2DlY2kldi4uSNViYEBvDi0lhyMwwsN0SOfrZHfTmc/A+o2AfWfkh6zn4Wz/ztEOj6DxQR3yVZBs9k7eqm45OP7VmGa9cky+BGnjDs3wAytdZ/7vj120C61vrHw91/rMP+q+omfvreeVq6BjBETWJ3uoE3VyQwbfITvsvoabWfwXPqd9BRAyFTYUrsqKdtCtfpQPNRsIU9wRZqAzSTNczQCuX+w03+TWvm1GiePWNj4VUNGtoiwCKHWp5I/5pZbP2HT8f0Z59k2I/X29fhpuR3/moqpd4B3gEwGAxjehJD1CRSEiPJzTCwbm702Jvxk6Jg7U9g9V/C5UNw9XPouTO2xxJOFw58D8jVmmO2To5aO2nVFncvSwD6GTj+DJzvsDK3op+I2xYCbO5elXcJmxbtkufx+t04Qgjhr57knf14XRt9CpirlJqllJoA7AI+GafnEkII8QjjshtHa21RSv0YOIT91Mvfa60rx+O5hBBCPNq4nXKitf4UGNtRByGEEE4liTshhPADMuyFEMIPyLAXQgg/IMNeCCH8gAx7IYTwAx6ROFZK3QZMY/zj04EWJy7HFWTNriFrdg1Zs2sMt2aj1vqxLsH1iGH/NJRSpx/3CjJPIWt2DVmza8iaXeNp1yy7cYQQwg/IsBdCCD/gC8P+t+5ewBjIml1D1uwasmbXeKo1e/0+eyGEEI/mC+/shRBCPIJXD3ulVKZS6pJS6qpS6ufuXs9wlFKJSqmvlVJVSqlKpdT/7rj9V0qpOqVUuePHdnevdSil1E2lVIVjbacdt0UppT5XSl1x/Bzp7nUOUkrNH7Ity5VSnUqpn3jadlZK/V4p1ayUujDktmG3q7L7J8fr+7xSarmHrPcflVLVjjV9qJSKcNyepJTqHbKt/83V6x1lzSO+DpRSv3Bs40tKqa0etOa9Q9Z7UylV7rh9bNtZa+2VP7Cnk68Bs4EJwDlgkbvXNcw6Y4Hljq+nYP8g9kXAr4D/w93rG2XdN4HpD9z2D8DPHV//HPgf7l7nKK+NRsDoadsZWAcsBy48arsC24HPsH/yWwZQ6iHr3QIEOb7+H0PWmzT0fh62jYd9HTj+Lp4DQoBZjpkS6AlrfuD3/y/g/3ya7ezN7+zTgKta6+ta6wFgD/Cym9f0EK11g9b6jOPru0AVEO/eVY3Zy8AfHF//AXjFjWsZzSbgmtZ6rBfqjRut9VGg9YGbR9quLwN/1HYlQIRSyqWf6D3cerXWh7W+97mQJUCCK9f0KCNs45G8DOzRWvdrrW8AV7HPFpcabc1KKQXsBAqf5jm8edjHA7eG/LoWDx+iSqkkIBUoddz0Y8e3wr/3pF0iDho4rJQqc3xeMMBMrXUD2P8RA2a4bXWj28V3/2J48naGkberN7zGf4D9u49Bs5RSZ5VSR5RSz7lrUSMY7nXgDdv4OaBJa31lyG1PvJ29edg/8kPNPYlSajLwPvATrXUn8K/AHCAFaMD+bZonWaO1Xg5sA/43pdQ6dy/ocTg+BvMlYJ/jJk/fzqPx6Ne4UuqXgAXId9zUABi01qnAXwEFSqmp7lrfA0Z6HXj0NnbI5rtvXsa0nb152NcCiUN+nQDUu2kto1JKBWMf9Pla6w8AtNZNWmur1toG/Adu+NZxNFrresfPzcCH2NfXNLgbwfFzs/tWOKJtwBmtdRN4/nZ2GGm7euxrXCn1PSALyNGOHcmOXSF3HF+XYd//Pc99q7xvlNeBx25jAKVUEPAasHfwtrFuZ28e9l7xoeaO/W2/A6q01v/3kNuH7nt9Fbjw4J91F6VUmFJqyuDX2A/IXcC+fb/nuNv3gI/ds8JRfeddkCdv5yFG2q6fAH/mOCsnA+gY3N3jTkqpTOBnwEta654ht0crpQIdX88G5gLX3bPK7xrldfAJsEspFaKUmoV9zSddvb5RbAaqtda1gzeMeTu7+qizk49gb8d+dss14JfuXs8Ia1yL/dvC80C548d24E9AheP2T4BYd691yJpnYz9D4RxQObhtgWnAl8AVx89R7l7rA+ueBNwBwofc5lHbGfs/RA2AGfu7yh+OtF2x72L4F8fruwJY6SHrvYp9P/fg6/nfHPd93fF6OQecAXZ40DYe8XUA/NKxjS8B2zxlzY7b/z/gf3ngvmPaznIFrRBC+AFv3o0jhBDiMcmwF0IIPyDDXggh/IAMeyGE8AMy7IUQwg/IsBdCCD8gw14IIfyADHshhPAD/z9WityPJdYiRQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f57c77bb048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_clf(clf5.coef_, clf5.theta_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "n = 10000\n",
    "X_train = pd.read_csv('multiclass_train_features_k' + str(k) + 'n' + str(n) + '.csv')\n",
    "y = pd.read_csv('multiclass_train_labels_k' + str(k) + 'n' + str(n) + '.csv')\n",
    "y_train = np.array(y.iloc[:, 0]).astype(int)\n",
    "\n",
    "X_test = pd.read_csv('multiclass_test_features_k' + str(k) + 'n' + str(n) + '.csv')\n",
    "y = pd.read_csv('multiclass_test_labels_k' + str(k) + 'n' + str(n) + '.csv')\n",
    "y_test = np.array(y.iloc[:, 0]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITs = []\n",
    "IT_predictions = []\n",
    "ATs = []\n",
    "AT_predictions = []\n",
    "\n",
    "s = 10\n",
    "for i in range(1, s):\n",
    "    a = i / s\n",
    "    clf5 = LogisticQuantileIT(gamma=a, alpha=1.)\n",
    "    clf5.fit(X_train, y_train)\n",
    "    ITpredictions = clf5.predict(X_test)\n",
    "    print('Mean Absolute Error of LogisticQuantileIT, gamma=' + str(a) + ' %s' %\n",
    "          metrics.mean_absolute_error(ITpredictions, y_test))\n",
    "    ITs.append(clf5)\n",
    "    IT_predictions.append(ITpredictions)\n",
    "\n",
    "    clf6 = LogisticQuantileAT(gamma=a, alpha=1.)\n",
    "    clf6.fit(X_train, y_train)\n",
    "    ATpredictions = clf6.predict(X_test)\n",
    "    print('Mean Absolute Error of LogisticQuantileAT, gamma=' + str(a) + ' %s' %\n",
    "          metrics.mean_absolute_error(ATpredictions, y_test))\n",
    "    ATs.append(clf6)\n",
    "    AT_predictions.append(ATpredictions)\n",
    "\n",
    "#     clf7 = LogisticQuantileDirect(gamma=a, alpha=1.)\n",
    "#     clf7.fit(X_train, y_train)\n",
    "#     print('Mean Absolute Error of LogisticQuantileDirect, gamma=' + str(a) + ' %s' %\n",
    "#           metrics.mean_absolute_error(clf7.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression for comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(clf.predict(X_test) != y_test.flatten()) / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(clf.predict(X_test), y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "full = np.zeros((4999, 2))\n",
    "full[:, 0] = clf.predict(X_test)\n",
    "full[:,1] = y_test\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OVA SVM\n",
    "from sklearn import svm\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(X_train, y_train.flatten()) \n",
    "\n",
    "print(sum(lin_clf.predict(X_test) != y_test.flatten()) / len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "full = np.zeros((4999, 2))\n",
    "full[:, 0] = AT_predictions[1]\n",
    "full[:,1] = y_test\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(AT_predictions[4] != y_test.flatten()) / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT_preds = np.zeros((4999, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    AT_preds[:, i] = AT_predictions[i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(scipy.stats.mode(AT_preds.T).mode!= y_test.flatten()) / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(scipy.stats.mode(AT_preds.T).mode, y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_preds = np.zeros((4999, 10))\n",
    "for i in range(1, 10):\n",
    "    IT_preds[:, i-1] = IT_predictions[i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(scipy.stats.mode(IT_preds.T).mode!= y_test.flatten()) / len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplex Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
