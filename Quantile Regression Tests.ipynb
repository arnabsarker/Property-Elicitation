{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From mord\n",
    "These first few cells are taken directly from the ordinal regression package, with some modifications for the logistic regression with weights for quantile elicitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import scipy\n",
    "from sklearn import base, metrics\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from classifiers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Elicitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_quantile(X_test, alpha):\n",
    "    ## Known from data generation\n",
    "    d = 2\n",
    "    w = np.array([1/d]*d) \n",
    "    portions = np.array([3, 7, 10, 15, 19, 21]) * 8\n",
    "    k = 6\n",
    "    \n",
    "    projection_points = X_test.dot(w) * d\n",
    "    return get_class_quantiles(projection_points, portions, k, alpha)\n",
    "    \n",
    "def get_class_quantiles(projection_points, portions, k, alpha):\n",
    "    y = np.matrix(np.zeros(projection_points.size)).T\n",
    "    for i in range(0, projection_points.size):\n",
    "        curr_point = projection_points[i]\n",
    "        likely_class = np.argmax((curr_point - portions) < 0)\n",
    "        if(likely_class == k - 1):\n",
    "            y[i] = k-1\n",
    "        else:\n",
    "            if(likely_class > 0):\n",
    "                prob_up = 1 - (portions[likely_class] - curr_point) / (portions[likely_class] - portions[likely_class - 1])\n",
    "            else:\n",
    "                prob_up = 1 - (portions[likely_class] - curr_point) / (portions[likely_class] - 0)\n",
    "            y[i] = likely_class + 1 * (prob_up > 1 - alpha)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of LogisticRegression: 1.7199\n",
      "0-1 Loss of LogisticRegression 0.8951\n",
      " \n",
      "Mean Absolute Error of LogisticAT 1.4566\n",
      "0-1 Loss of LogisticAT 0.8588\n",
      " \n",
      "Mean Absolute Error of LogisticIT 1.5577\n",
      "0-1 Loss of LogisticIT 0.7884\n",
      " \n",
      "Mean Absolute Error of LogisticSE 1.3202\n",
      "0-1 Loss of LogisticSE 0.8961\n",
      " \n",
      "Mean Absolute Error of LogisticQuantileIT, gamma=0.8 1.011\n",
      "0-1 Loss of LogisticQuantileIT, gamma=0.8 0.7807\n",
      " \n",
      "Mean Absolute Error of LogisticQuantileAT, gamma=0.8 0.6455\n",
      "0-1 Loss of LogisticQuantileAT, gamma=0.8 0.4797\n",
      " \n",
      "Mean Absolute Error of LogisticQuantileDirect, gamma=0.8 1.6558\n",
      "0-1 Loss of LogisticQuantileDirect, gamma=0.8 0.9645\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Quantile to be elicited\n",
    "a = 0.8\n",
    "\n",
    "X_train = pd.read_csv('quantile_synthetic_features.csv', names=['dim1', 'dim2'])\n",
    "y = pd.read_csv('quantile_synthetic_labels.csv', names=['y'])\n",
    "y_train = np.array(y.iloc[:, 0]).astype(int)\n",
    "\n",
    "X_test = pd.read_csv('quantile_synthetic_manytestfeatures.csv', names=['dim1', 'dim2'])\n",
    "y_test = pd.read_csv('quantile_synthetic_manytestlabels.csv', names=['y'])\n",
    "y_test = np.array(y_test.iloc[:, 0]).astype(int)\n",
    "\n",
    "clf1 = linear_model.LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    multi_class='multinomial')\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "print('Mean Absolute Error of LogisticRegression: %s' %\n",
    "      metrics.mean_absolute_error(clf1.predict(X_test), y_test))\n",
    "print('0-1 Loss of LogisticRegression %s' %\n",
    "      metrics.zero_one_loss(clf1.predict(X_test), y_test))\n",
    "print(' ')\n",
    "\n",
    "clf2 = LogisticAT(alpha=1.)\n",
    "clf2.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticAT %s' %\n",
    "      metrics.mean_absolute_error(clf2.predict(X_test), y_test))\n",
    "print('0-1 Loss of LogisticAT %s' %\n",
    "      metrics.zero_one_loss(clf2.predict(X_test), y_test))\n",
    "print(' ')\n",
    "\n",
    "clf3 = LogisticIT(alpha=1.)\n",
    "clf3.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticIT %s' %\n",
    "      metrics.mean_absolute_error(clf3.predict(X_test), y_test))\n",
    "print('0-1 Loss of LogisticIT %s' %\n",
    "      metrics.zero_one_loss(clf3.predict(X_test), y_test))\n",
    "print(' ')\n",
    "\n",
    "clf4 = LogisticSE(alpha=1.)\n",
    "clf4.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticSE %s' %\n",
    "      metrics.mean_absolute_error(clf4.predict(X_test), y_test))\n",
    "print('0-1 Loss of LogisticSE %s' %\n",
    "      metrics.zero_one_loss(clf4.predict(X_test), y_test))\n",
    "print(' ')\n",
    "\n",
    "clf5 = LogisticQuantileIT(gamma=a, alpha=1.)\n",
    "clf5.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticQuantileIT, gamma=' + str(a) + ' %s' %\n",
    "      metrics.mean_absolute_error(clf5.predict(X_test), y_test))\n",
    "print('0-1 Loss of LogisticQuantileIT, gamma=' + str(a) + ' %s' %\n",
    "      metrics.zero_one_loss(clf5.predict(X_test), y_test))\n",
    "print(' ')\n",
    "\n",
    "clf6 = LogisticQuantileAT(gamma=a, alpha=1.)\n",
    "clf6.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticQuantileAT, gamma=' + str(a) + ' %s' %\n",
    "      metrics.mean_absolute_error(clf6.predict(X_test), y_test))\n",
    "print('0-1 Loss of LogisticQuantileAT, gamma=' + str(a) + ' %s' %\n",
    "      metrics.zero_one_loss(clf6.predict(X_test), y_test))\n",
    "print(' ')\n",
    "\n",
    "clf7 = LogisticQuantileDirect(gamma=a, alpha=1.)\n",
    "clf7.fit(X_train, y_train)\n",
    "print('Mean Absolute Error of LogisticQuantileDirect, gamma=' + str(a) + ' %s' %\n",
    "      metrics.mean_absolute_error(clf7.predict(X_test), y_test))\n",
    "print('0-1 Loss of LogisticQuantileDirect, gamma=' + str(a) + ' %s' %\n",
    "      metrics.zero_one_loss(clf7.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of LogisticQuantileAT, gamma=0.1 4.2673\n",
      "0-1 Loss of LogisticQuantileAT, gamma=0.1 0.9132\n",
      "Mean Absolute Error of LogisticQuantileAT, gamma=0.2 3.3893\n",
      "0-1 Loss of LogisticQuantileAT, gamma=0.2 0.8998\n",
      "Mean Absolute Error of LogisticQuantileAT, gamma=0.3 2.8346\n",
      "0-1 Loss of LogisticQuantileAT, gamma=0.3 0.8753\n",
      "Mean Absolute Error of LogisticQuantileAT, gamma=0.4 2.5093\n",
      "0-1 Loss of LogisticQuantileAT, gamma=0.4 0.87\n",
      "Mean Absolute Error of LogisticQuantileAT, gamma=0.5 2.4259\n",
      "0-1 Loss of LogisticQuantileAT, gamma=0.5 0.8729\n",
      "Mean Absolute Error of LogisticQuantileAT, gamma=0.6 2.5682\n",
      "0-1 Loss of LogisticQuantileAT, gamma=0.6 0.8837\n",
      "Mean Absolute Error of LogisticQuantileAT, gamma=0.7 2.9424\n",
      "0-1 Loss of LogisticQuantileAT, gamma=0.7 0.9017\n",
      "Mean Absolute Error of LogisticQuantileAT, gamma=0.8 3.5594\n",
      "0-1 Loss of LogisticQuantileAT, gamma=0.8 0.9222\n",
      "Mean Absolute Error of LogisticQuantileAT, gamma=0.9 4.5461\n",
      "0-1 Loss of LogisticQuantileAT, gamma=0.9 0.9312\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('quantile_synthetic_features.csv', names=['dim1', 'dim2'])\n",
    "y = pd.read_csv('quantile_synthetic_labels.csv', names=['y'])\n",
    "y_train = np.array(y.iloc[:, 0]).astype(int)\n",
    "\n",
    "X_test = pd.read_csv('quantile_synthetic_testfeatures.csv', names=['dim1', 'dim2'])\n",
    "y_test = pd.read_csv('quantile_synthetic_testlabels.csv', names=['y'])\n",
    "y_test = np.array(y_test.iloc[:, 0]).astype(int)\n",
    "\n",
    "\n",
    "ATs = []\n",
    "AT_predictions = []\n",
    "\n",
    "s = 10 # Number of quantiles\n",
    "for i in range(1, s):\n",
    "    a = i/s\n",
    "\n",
    "    clf6 = LogisticQuantileAT(gamma=a, alpha=1.)\n",
    "    clf6.fit(X_train, y_train)\n",
    "    ATpredictions = clf6.predict(X_test)\n",
    "    print('Mean Absolute Error of LogisticQuantileAT, gamma=' + str(a) + ' %s' %\n",
    "          metrics.mean_absolute_error(ATpredictions, y_test))\n",
    "    print('0-1 Loss of LogisticQuantileAT, gamma=' + str(a) + ' %s' %\n",
    "          metrics.zero_one_loss(ATpredictions, y_test))\n",
    "    ATs.append(clf6)\n",
    "    AT_predictions.append(ATpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT_preds = np.zeros((10000, 10))\n",
    "for i in range(1, 10):\n",
    "    AT_preds[:, i] = AT_predictions[i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46516"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scipy.stats.mode(AT_preds.T).mode[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0864\n",
      "0.886\n"
     ]
    }
   ],
   "source": [
    "print(metrics.mean_absolute_error(scipy.stats.mode(AT_preds.T).mode[0], y_test.flatten()))\n",
    "print(metrics.zero_one_loss(scipy.stats.mode(AT_preds.T).mode[0], y_test.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of LogisticRegression: 2.8393\n",
      "0-1 Loss of LogisticRegression 0.8298\n",
      " \n"
     ]
    }
   ],
   "source": [
    "clf1 = linear_model.LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    multi_class='multinomial')\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "print('Mean Absolute Error of LogisticRegression: %s' %\n",
    "      metrics.mean_absolute_error(clf1.predict(X_test), y_test))\n",
    "print('0-1 Loss of LogisticRegression %s' %\n",
    "      metrics.zero_one_loss(clf1.predict(X_test), y_test))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Threshold Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_clf(coef, theta):\n",
    "    ## From data generation\n",
    "    last_point = 170\n",
    "    \n",
    "    x = np.arange(0, 170, 0.5)\n",
    "    y = x * (coef[0] / coef[1])\n",
    "    plt.plot(x,y)\n",
    "    \n",
    "    for t in theta:\n",
    "        t_y = x * (-coef[1] / coef[0]) + t * (2 / coef[0])\n",
    "        t_y = t_y * (t_y > 0)\n",
    "        plt.plot(x, t_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_clf(clf5.coef_, clf5.theta_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "n = 10000\n",
    "X_train = pd.read_csv('multiclass_train_features_k' + str(k) + 'n' + str(n) + '.csv')\n",
    "y = pd.read_csv('multiclass_train_labels_k' + str(k) + 'n' + str(n) + '.csv')\n",
    "y_train = np.array(y.iloc[:, 0]).astype(int)\n",
    "\n",
    "X_test = pd.read_csv('multiclass_test_features_k' + str(k) + 'n' + str(n) + '.csv')\n",
    "y = pd.read_csv('multiclass_test_labels_k' + str(k) + 'n' + str(n) + '.csv')\n",
    "y_test = np.array(y.iloc[:, 0]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITs = []\n",
    "IT_predictions = []\n",
    "ATs = []\n",
    "AT_predictions = []\n",
    "\n",
    "s = 10\n",
    "for i in range(1, s):\n",
    "    a = i / s\n",
    "    clf5 = LogisticQuantileIT(gamma=a, alpha=1.)\n",
    "    clf5.fit(X_train, y_train)\n",
    "    ITpredictions = clf5.predict(X_test)\n",
    "    print('Mean Absolute Error of LogisticQuantileIT, gamma=' + str(a) + ' %s' %\n",
    "          metrics.mean_absolute_error(ITpredictions, y_test))\n",
    "    ITs.append(clf5)\n",
    "    IT_predictions.append(ITpredictions)\n",
    "\n",
    "    clf6 = LogisticQuantileAT(gamma=a, alpha=1.)\n",
    "    clf6.fit(X_train, y_train)\n",
    "    ATpredictions = clf6.predict(X_test)\n",
    "    print('Mean Absolute Error of LogisticQuantileAT, gamma=' + str(a) + ' %s' %\n",
    "          metrics.mean_absolute_error(ATpredictions, y_test))\n",
    "    ATs.append(clf6)\n",
    "    AT_predictions.append(ATpredictions)\n",
    "\n",
    "#     clf7 = LogisticQuantileDirect(gamma=a, alpha=1.)\n",
    "#     clf7.fit(X_train, y_train)\n",
    "#     print('Mean Absolute Error of LogisticQuantileDirect, gamma=' + str(a) + ' %s' %\n",
    "#           metrics.mean_absolute_error(clf7.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression for comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(clf.predict(X_test) != y_test.flatten()) / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(clf.predict(X_test), y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "full = np.zeros((4999, 2))\n",
    "full[:, 0] = clf.predict(X_test)\n",
    "full[:,1] = y_test\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OVA SVM\n",
    "from sklearn import svm\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(X_train, y_train.flatten()) \n",
    "\n",
    "print(sum(lin_clf.predict(X_test) != y_test.flatten()) / len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "full = np.zeros((4999, 2))\n",
    "full[:, 0] = AT_predictions[1]\n",
    "full[:,1] = y_test\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(AT_predictions[4] != y_test.flatten()) / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT_preds = np.zeros((4999, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    AT_preds[:, i] = AT_predictions[i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(scipy.stats.mode(AT_preds.T).mode!= y_test.flatten()) / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(scipy.stats.mode(AT_preds.T).mode, y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_preds = np.zeros((4999, 10))\n",
    "for i in range(1, 10):\n",
    "    IT_preds[:, i-1] = IT_predictions[i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(scipy.stats.mode(IT_preds.T).mode!= y_test.flatten()) / len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplex Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
